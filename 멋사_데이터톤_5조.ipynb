{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iljeong/DATATHON_5/blob/main/%EB%A9%8B%EC%82%AC_%EB%8D%B0%EC%9D%B4%ED%84%B0%ED%86%A4_5%EC%A1%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gcar/Socar ë¦¬ë·° í¬ë¡¤ë§\n",
        "- Google Play Store ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘\n",
        "- Apple App Store ë¦¬ë·° ë°ì´í„° ìˆ˜ì§‘"
      ],
      "metadata": {
        "id": "Dn6mdGMtDnei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "from google_play_scraper import Sort, reviews\n",
        "\n",
        "# ==============================\n",
        "# Google Play ë¦¬ë·° ìˆ˜ì§‘\n",
        "# ==============================\n",
        "def crawl_google_reviews(app_id, provider_name, lang='ko', country='kr', max_count=3000):\n",
        "    \"\"\"\n",
        "    Google Play ë¦¬ë·° í¬ë¡¤ë§\n",
        "    - app_id: Google Play ì•± ID (ì˜ˆ: 'com.greencar')\n",
        "    - provider_name: 'Gcar' ë˜ëŠ” 'Socar'\n",
        "    - max_count: ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜\n",
        "    \"\"\"\n",
        "    all_reviews = []\n",
        "    next_token = None\n",
        "    total = 0\n",
        "\n",
        "    print(f\"\\nğŸ“± [Google Play] {provider_name} ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...\")\n",
        "\n",
        "    while True:\n",
        "        rv_list, next_token = reviews(\n",
        "            app_id,\n",
        "            lang=lang,\n",
        "            country=country,\n",
        "            sort=Sort.NEWEST,\n",
        "            continuation_token=next_token\n",
        "        )\n",
        "\n",
        "        if not rv_list:\n",
        "            break\n",
        "\n",
        "        for r in rv_list:\n",
        "            rating = r.get(\"score\")\n",
        "            review_text = r.get(\"content\")\n",
        "            author = r.get(\"userName\")\n",
        "            version = r.get(\"reviewCreatedVersion\")\n",
        "            at = r.get(\"at\")\n",
        "\n",
        "            if at is not None:\n",
        "                review_date = at.strftime(\"%Y-%m-%d %H:%M\")\n",
        "            else:\n",
        "                review_date = None\n",
        "\n",
        "            all_reviews.append({\n",
        "                \"provider\": provider_name,\n",
        "                \"store\": \"Google Play Store\",\n",
        "                \"rating\": rating,\n",
        "                \"review_text\": review_text,\n",
        "                \"review_date\": review_date,\n",
        "                \"author\": author,\n",
        "                \"version\": version,\n",
        "            })\n",
        "\n",
        "        total += len(rv_list)\n",
        "        print(f\"  â†’ ëˆ„ì  {total}ê°œ ìˆ˜ì§‘\")\n",
        "\n",
        "        if not next_token or total >= max_count:\n",
        "            break\n",
        "\n",
        "        time.sleep(1.5)\n",
        "\n",
        "    df = pd.DataFrame(all_reviews)\n",
        "    print(f\"[Google Play] {provider_name} ìµœì¢… ìˆ˜ì§‘ ê°œìˆ˜: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Apple App Store ë¦¬ë·° ìˆ˜ì§‘\n",
        "# ==============================\n",
        "def crawl_appstore_reviews(app_id, provider_name, country='kr', max_page=20, sort='recent'):\n",
        "    \"\"\"\n",
        "    App Store ë¦¬ë·° í¬ë¡¤ë§ (RSS)\n",
        "    - app_id: App Store ì•± ID (ìˆ«ì ë¬¸ìì—´)\n",
        "    - provider_name: 'Gcar' ë˜ëŠ” 'Socar'\n",
        "    - country: 'kr' (í•œêµ­)\n",
        "    - sort: 'recent'(ìµœì‹ ìˆœ) / 'helpful'(ë„ì›€ìˆœ) ë“±\n",
        "    \"\"\"\n",
        "    print(f\"\\nğŸ [App Store] {provider_name} ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘... (ì •ë ¬: {sort})\")\n",
        "\n",
        "    base_url = (\n",
        "        f\"https://itunes.apple.com/{country}/rss/customerreviews/\"\n",
        "        f\"page={{page}}/id={app_id}/sortby={sort}/json\"\n",
        "    )\n",
        "\n",
        "    all_reviews = []\n",
        "    for page in range(1, max_page + 1):\n",
        "        url = base_url.format(page=page)\n",
        "        resp = requests.get(url)\n",
        "        if resp.status_code != 200:\n",
        "            print(f\"  âš ï¸ page {page}: HTTP {resp.status_code}, ì¤‘ë‹¨\")\n",
        "            break\n",
        "\n",
        "        data = resp.json()\n",
        "        entries = data.get(\"feed\", {}).get(\"entry\", [])\n",
        "\n",
        "        if len(entries) <= 1:\n",
        "            print(\"  âš ï¸ ë” ì´ìƒ ë¦¬ë·° ì—†ìŒ, ì¤‘ë‹¨\")\n",
        "            break\n",
        "\n",
        "        for entry in entries[1:]:\n",
        "            rating = int(entry.get(\"im:rating\", {}).get(\"label\", 0))\n",
        "            review_text = entry.get(\"content\", {}).get(\"label\")\n",
        "            author = entry.get(\"author\", {}).get(\"name\", {}).get(\"label\")\n",
        "            version = entry.get(\"im:version\", {}).get(\"label\")\n",
        "            updated = entry.get(\"updated\", {}).get(\"label\")\n",
        "\n",
        "            if updated:\n",
        "                try:\n",
        "                    dt = pd.to_datetime(updated)\n",
        "                    review_date = dt.strftime(\"%Y-%m-%d %H:%M\")\n",
        "                except Exception:\n",
        "                    review_date = updated\n",
        "            else:\n",
        "                review_date = None\n",
        "\n",
        "            all_reviews.append({\n",
        "                \"provider\": provider_name,\n",
        "                \"store\": \"App Store\",\n",
        "                \"rating\": rating,\n",
        "                \"review_text\": review_text,\n",
        "                \"review_date\": review_date,\n",
        "                \"author\": author,\n",
        "                \"version\": version,\n",
        "            })\n",
        "\n",
        "        print(f\"  â†’ page {page} ì²˜ë¦¬, ëˆ„ì  {len(all_reviews)}ê°œ\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    df = pd.DataFrame(all_reviews)\n",
        "    print(f\"[App Store] {provider_name} ìµœì¢… ìˆ˜ì§‘ ê°œìˆ˜: {len(df)}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# ì‹¤ì œ ì‹¤í–‰ ë¶€ë¶„\n",
        "# ==============================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    google_apps = {\n",
        "        \"Gcar\": \"com.greencar\",\n",
        "        \"Socar\": \"kr.co.socar\",\n",
        "    }\n",
        "\n",
        "    appstore_apps = {\n",
        "        \"Gcar\": \"834786891\",\n",
        "        \"Socar\": \"869016361\",\n",
        "    }\n",
        "\n",
        "    # ---------- Google Play ----------\n",
        "    google_frames = []\n",
        "    for provider, app_id in google_apps.items():\n",
        "        df_g = crawl_google_reviews(\n",
        "            app_id=app_id,\n",
        "            provider_name=provider,\n",
        "            max_count=20000\n",
        "        )\n",
        "        google_frames.append(df_g)\n",
        "\n",
        "    # ---------- App Store ----------\n",
        "    appstore_frames = []\n",
        "    for provider, app_id in appstore_apps.items():\n",
        "        df_a = crawl_appstore_reviews(\n",
        "            app_id=app_id,\n",
        "            provider_name=provider,\n",
        "            sort=\"recent\",\n",
        "            max_page=50\n",
        "        )\n",
        "        appstore_frames.append(df_a)\n",
        "\n",
        "    # ==============================\n",
        "    # ë°ì´í„° ë³‘í•© + ID ë¶€ì—¬ + ì»¬ëŸ¼ ì •ë¦¬\n",
        "    # ==============================\n",
        "    all_df = pd.concat(google_frames + appstore_frames, ignore_index=True)\n",
        "\n",
        "    all_df[\"review_date\"] = all_df[\"review_date\"].astype(str)\n",
        "\n",
        "    all_df.insert(\n",
        "        0,\n",
        "        \"ID\",\n",
        "        [\"review_\" + str(i).zfill(2) for i in range(1, len(all_df) + 1)]\n",
        "    )\n",
        "\n",
        "    # ê°ì •ë¶„ì„ / ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ í˜•ì‹ ë§Œë“¤ì–´ë‘ê¸°\n",
        "    all_df[\"Sentiment_label\"] = None\n",
        "    all_df[\"Sentiment_score\"] = None\n",
        "    all_df[\"Category\"] = None\n",
        "\n",
        "    # ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ\n",
        "    final_cols = [\n",
        "        \"ID\",\n",
        "        \"provider\",\n",
        "        \"store\",\n",
        "        \"rating\",\n",
        "        \"review_text\",\n",
        "        \"review_date\",\n",
        "        \"author\",\n",
        "        \"version\",\n",
        "        \"Sentiment_label\",\n",
        "        \"Sentiment_score\",\n",
        "        \"Category\",\n",
        "    ]\n",
        "    all_df = all_df[final_cols]\n",
        "\n",
        "    # ==============================\n",
        "    # ì €ì¥\n",
        "    # ==============================\n",
        "    output_path = \"reviews_sentiment_all.csv\"\n",
        "    all_df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"\\nğŸ‰ ì™„ë£Œ! â†’ {output_path} ë¡œ ì €ì¥ë¨\")"
      ],
      "metadata": {
        "id": "CE4Ol5nLEAed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì¹´ì¹´ì˜¤ ë§µ API í˜¸ì¶œ\n",
        "- ì°¨ê³ ì§€ ë°ì´í„° ìˆ˜ì§‘"
      ],
      "metadata": {
        "id": "KC2NXnDKDKmN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlpS_3xkDA5U"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# ì „êµ­ ì˜ì¹´ì¡´ / ì§€ì¹´ì¡´ ì™„ì „ ìˆ˜ì§‘ (ëˆ„ë½ ìµœì†Œí™”, ì¤‘ë³µ ì œê±° ì—†ìŒ)\n",
        "# ==========================================================\n",
        "\n",
        "REST_API_KEY = \"d47532baa79392c04b6c3cb99713a66f\"\n",
        "\n",
        "# ì£¼ìš” í‚¤ì›Œë“œ (ëŒ€ì†Œë¬¸ìÂ·í‘œê¸° ë³€í˜• í¬í•¨)\n",
        "KEYWORDS = [\"G car zone\"]\n",
        "\n",
        "# Kakao Local API í•œê³„\n",
        "RADIUS_M = 20000      # ìµœëŒ€ ë°˜ê²½ 20km\n",
        "GRID_STEP_KM = 12     # 12km ê°„ê²© ê²©ì (ì´˜ì´˜í•˜ê²Œ ì „êµ­ ì»¤ë²„)\n",
        "\n",
        "# ëŒ€í•œë¯¼êµ­ ëŒ€ëµ ê²½ê³„\n",
        "MIN_LAT, MAX_LAT = 33.10, 38.70\n",
        "MIN_LNG, MAX_LNG = 124.60, 131.95\n",
        "# ==========================================================\n",
        "\n",
        "import os, math, time, json, requests, pandas as pd, pathlib\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ì¸ì¦ í—¤ë” ì„¸íŒ…\n",
        "os.environ[\"KAKAO_REST_KEY\"] = REST_API_KEY\n",
        "HEADERS = {\"Authorization\": f\"KakaoAK {os.environ['KAKAO_REST_KEY']}\"}\n",
        "\n",
        "# ======== ë„ìš°ë¯¸ í•¨ìˆ˜ ========\n",
        "def km_to_deg_lat(km): return km / 111.0\n",
        "def km_to_deg_lng(km, lat): return km / (111.0 * math.cos(math.radians(lat)))\n",
        "\n",
        "def build_grid(min_lat, max_lat, min_lng, max_lng, step_km):\n",
        "    points = []\n",
        "    dlat = km_to_deg_lat(step_km)\n",
        "    lat = min_lat\n",
        "    while lat <= max_lat:\n",
        "        dlng = km_to_deg_lng(step_km, lat)\n",
        "        lng = min_lng\n",
        "        while lng <= max_lng:\n",
        "            points.append((round(lat, 6), round(lng, 6)))\n",
        "            lng += dlng\n",
        "        lat += dlat\n",
        "    return points\n",
        "\n",
        "def search_keyword_all(query, lat, lng, radius=RADIUS_M, size=15, max_pages=45, pause=0.12):\n",
        "    url = \"https://dapi.kakao.com/v2/local/search/keyword.json\"\n",
        "    out = []\n",
        "    for page in range(1, max_pages + 1):\n",
        "        params = {\"query\": query, \"y\": lat, \"x\": lng, \"radius\": radius, \"size\": size, \"page\": page}\n",
        "        r = requests.get(url, params=params, headers=HEADERS, timeout=15)\n",
        "        r.raise_for_status()\n",
        "        data = r.json()\n",
        "        out.extend(data.get(\"documents\", []))\n",
        "        if data.get(\"meta\", {}).get(\"is_end\"): break\n",
        "        time.sleep(pause)\n",
        "    return out\n",
        "\n",
        "# ======== ê²©ì ìƒì„± ========\n",
        "grid_points = build_grid(MIN_LAT, MAX_LAT, MIN_LNG, MAX_LNG, GRID_STEP_KM)\n",
        "print(f\"ê²©ì ì¤‘ì‹¬ì  ê°œìˆ˜: {len(grid_points)}  (ì „êµ­ ì»¤ë²„ ì•½ {len(grid_points)*RADIUS_M/1000/GRID_STEP_KM:.1f}%)\")\n",
        "\n",
        "# ======== ìˆ˜ì§‘ ì‹œì‘ ========\n",
        "rows = []\n",
        "for (lat, lng) in tqdm(grid_points):\n",
        "    for kw in KEYWORDS:\n",
        "        try:\n",
        "            docs = search_keyword_all(kw, lat, lng)\n",
        "        except Exception as e:\n",
        "            continue\n",
        "        for d in docs:\n",
        "            rows.append({\n",
        "                \"query\": kw,\n",
        "                \"grid_lat\": lat,\n",
        "                \"grid_lng\": lng,\n",
        "                \"name\": d.get(\"place_name\"),\n",
        "                \"category\": d.get(\"category_name\"),\n",
        "                \"address\": d.get(\"address_name\"),\n",
        "                \"road_address\": d.get(\"road_address_name\"),\n",
        "                \"lat\": float(d[\"y\"]) if d.get(\"y\") else None,\n",
        "                \"lng\": float(d[\"x\"]) if d.get(\"x\") else None,\n",
        "                \"distance_m\": int(d[\"distance\"]) if d.get(\"distance\") else None,\n",
        "                \"phone\": d.get(\"phone\"),\n",
        "                \"url\": d.get(\"place_url\")\n",
        "            })\n",
        "    time.sleep(0.2)\n",
        "\n",
        "# ======== ê²°ê³¼ ì €ì¥ ========\n",
        "df = pd.DataFrame(rows)\n",
        "print(f\"\\nâœ… ì´ {len(df)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ (ì¤‘ë³µ í¬í•¨)\")\n",
        "\n",
        "ROOT = pathlib.Path(\"kakao_output_nationwide\")\n",
        "ROOT.mkdir(exist_ok=True)\n",
        "csv_path  = ROOT / \"kakao_places_socar_gcar_nationwide.csv\"\n",
        "json_path = ROOT / \"kakao_places_socar_gcar_nationwide.json\"\n",
        "\n",
        "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "df.to_json(json_path, orient=\"records\", force_ascii=False, indent=2)\n",
        "print(f\"\\nì €ì¥ ì™„ë£Œ âœ…\\nCSV : {csv_path}\\nJSON: {json_path}\")\n",
        "\n",
        "# ì§€ì—­ ìš”ì•½ í™•ì¸\n",
        "if not df.empty:\n",
        "    df[\"region\"] = df[\"address\"].fillna(\"\").apply(lambda s: s.split()[0] if s else \"\")\n",
        "    print(\"\\nğŸ“Š ì§€ì—­ë³„ ë¶„í¬ Top 15\")\n",
        "    print(df[\"region\"].value_counts().head(15))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬\n",
        "- ë‹¤ë¥¸ ëª¨ë¹Œë¦¬í‹° ì„œë¹„ìŠ¤(ì¼ë ˆí´/ë”œì¹´/íŒ¨ìŠ¤ì¹´/ì¹´ì¹´ì˜¤/ëª¨ë¹Œë¦¬í‹°) ì–¸ê¸‰ ë¦¬ë·° ì œì™¸\n",
        "- ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì œê±°\n",
        "- ê³µë°± ì •ë¦¬\n",
        "- Okt ëª…ì‚¬ ì¶”ì¶œ(ì•ˆ ë˜ë©´ ì •ê·œì‹ í† í°í™”)\n",
        "- ë¶ˆìš©ì–´ ì œê±°"
      ],
      "metadata": {
        "id": "Kt1I7QmDFOFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Java ì„¤ì¹˜\n",
        "!apt-get update\n",
        "!apt-get install -y openjdk-11-jdk\n",
        "\n",
        "# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] += os.pathsep + os.path.join(os.environ[\"JAVA_HOME\"], \"bin\")\n",
        "\n",
        "# konlpy ì„¤ì¹˜\n",
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "KUdUWz_nFQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ê¸°ë³¸ ì…‹ì—…\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ì¬í˜„ì„±(ëœë¤ ê³ ì •)\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "# ì…ë ¥ / ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
        "INPUT_PATH = \"/content/CarSharing_Reviews_20251029_id.csv\"\n",
        "OUTPUT_PATH = \"/content/reviews_preprocessed.csv\"\n",
        "\n",
        "\n",
        "# 2. í† í¬ë‚˜ì´ì €: Okt â†’ ì•ˆë˜ë©´ í´ë°±\n",
        "def get_tokenizer():\n",
        "    try:\n",
        "        from konlpy.tag import Okt\n",
        "        okt = Okt()\n",
        "\n",
        "        def tokenize_ko(text: str):\n",
        "            text = str(text)\n",
        "            morphs = []\n",
        "            for w, pos in okt.pos(text, norm=True, stem=True):\n",
        "                # ëª…ì‚¬ë§Œ ì‚¬ìš© (í•„ìš”í•˜ë©´ í˜•ìš©ì‚¬/ë™ì‚¬ë„ ì¶”ê°€ ê°€ëŠ¥)\n",
        "                if pos == \"Noun\":\n",
        "                    # í•œ ê¸€ì ì§œë¦¬ ëª…ì‚¬ëŠ” ë…¸ì´ì¦ˆê°€ ë§ì•„ì„œ ì œê±°\n",
        "                    if len(w) > 1:\n",
        "                        morphs.append(w)\n",
        "            return morphs\n",
        "\n",
        "        print(\"[INFO] í† í¬ë‚˜ì´ì €: Okt ì‚¬ìš©\")\n",
        "        return tokenize_ko\n",
        "\n",
        "    except Exception as e:\n",
        "        # JVM ì„¤ì¹˜ ì•ˆ ë˜ì–´ ìˆê±°ë‚˜ konlpy ë¬¸ì œ ìˆì„ ë•Œ\n",
        "        print(f\"[WARN] Okt ì‚¬ìš© ë¶ˆê°€, fallback í† í¬ë‚˜ì´ì € ì‚¬ìš©: {e}\")\n",
        "\n",
        "        def simple_tokenize_ko(text: str):\n",
        "            text = re.sub(r\"[^ê°€-í£A-Za-z0-9\\s]\", \" \", str(text))\n",
        "            # í•œê¸€ 2ê¸€ì ì´ìƒë§Œ ë‚¨ê¸°ê¸°\n",
        "            toks = re.findall(r\"[ê°€-í£]{2,}\", text)\n",
        "            return toks\n",
        "\n",
        "        return simple_tokenize_ko\n",
        "\n",
        "\n",
        "tokenize_ko = get_tokenizer()\n",
        "\n",
        "\n",
        "# 3. ë°ì´í„° ë¡œë“œ\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "print(f\"[INFO] ì›ë³¸ ë¦¬ë·° ìˆ˜: {len(df):,}ê±´\")\n",
        "\n",
        "# í•„ìˆ˜ ì»¬ëŸ¼ ì²´í¬ (review_text ê¸°ì¤€)\n",
        "assert \"review_text\" in df.columns, \"CSVì— 'review_text' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# ë‚ ì§œ ì»¬ëŸ¼ ìë™ íƒìƒ‰\n",
        "date_col = None\n",
        "for c in df.columns:\n",
        "    if \"date\" in c.lower():\n",
        "        date_col = c\n",
        "        break\n",
        "print(f\"[INFO] ë‚ ì§œ ì»¬ëŸ¼ ìë™ íƒì§€ ê²°ê³¼: {date_col}\")\n",
        "\n",
        "\n",
        "# 4. íƒ€ ì„œë¹„ìŠ¤ ì–¸ê¸‰ ì œê±°\n",
        "#    (ì¼ë ˆí´, ë”œì¹´, íŒ¨ìŠ¤ì¹´, ì¹´ì¹´ì˜¤, ëª¨ë¹Œë¦¬í‹° ë“±)\n",
        "exclude_keywords = ['ì¼ë ˆí´', 'ë”œì¹´', 'íŒ¨ìŠ¤ì¹´', 'ì¹´ì¹´ì˜¤', 'ëª¨ë¹Œë¦¬í‹°']\n",
        "\n",
        "# ì¡°ì‚¬/ì–´ë¯¸ ë¶™ì€ ê²½ìš°(ì¼ë ˆí´ì€, ì¹´ì¹´ì˜¤ëŠ”, ëª¨ë¹Œë¦¬í‹°ì—ì„œ ë“±)ê¹Œì§€ ê°™ì´ ì œê±°\n",
        "pattern = r'(' + '|'.join(map(re.escape, exclude_keywords)) + r')(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ì™€|ê³¼|ì˜|ë„)?'\n",
        "\n",
        "before = len(df)\n",
        "df = df[~df['review_text'].astype(str).str.contains(pattern, case=False, na=False)].copy()\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "print(f\"[INFO] íƒ€ ì„œë¹„ìŠ¤ ì–¸ê¸‰ ì œê±°: {before - len(df)}ê±´ ì œê±°, ì”ì—¬ {len(df):,}ê±´\")\n",
        "\n",
        "\n",
        "# 5. í…ìŠ¤íŠ¸ ì •ë¦¬ (íŠ¹ìˆ˜ë¬¸ì ì œê±° + ê³µë°± ì •ë¦¬)\n",
        "def clean_text(text: str) -> str:\n",
        "    text = str(text)\n",
        "    # ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì œê±°(í•œê¸€,ì˜ë¬¸,ìˆ«ì,ê³µë°±ë§Œ ë‚¨ê¸°ê¸°)\n",
        "    text = re.sub(r\"[^ê°€-í£A-Za-z0-9\\s]\", \" \", text)\n",
        "    # ì—°ì† ê³µë°± â†’ í•œ ì¹¸\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "df[\"clean_text\"] = df[\"review_text\"].astype(str).apply(clean_text)\n",
        "\n",
        "\n",
        "# 6. ë¶ˆìš©ì–´ ì •ì˜\n",
        "STOPWORDS = set([\n",
        "    \"í•˜ë‹¤\",\"ë˜ë‹¤\",\"ì´ë‹¤\",\"ìˆë‹¤\",\"ì—†ë‹¤\",\"ê°™ë‹¤\",\"ë³´ë‹¤\",\"ì£¼ë‹¤\",\"ë°›ë‹¤\",\"ë˜\",\n",
        "    \"ì¢‹ë‹¤\",\"ë‚˜ì˜ë‹¤\",\"ìë‹¤\",\"ë¨\",\"ë˜ê³ \",\"í•´ì„œ\",\"í•˜ë©´\",\"í•˜ëŠ”\",\"í–ˆë‹¤\",\"í–ˆë˜\",\n",
        "    \"ê°™ì€\",\"ë“ \",\"ë‹¤ì‹œ\",\"ì˜ˆ\",\"ì•„ì´ê³ \",\"ì•„íœ´\",\"í•˜\",\"í—ˆ\",\"í›„\",\n",
        "    \"ì˜\",\"ê°€\",\"ì´\",\"ì€\",\"ëŠ”\",\"ì„\",\"ë¥¼\",\"ì—\",\"ë¡œ\",\"ë„\",\"ë§Œ\",\"ì™€\",\"ê³¼\",\"ë°\",\n",
        "    \"ê·¸ë¦¬ê³ \",\"ë§ˆë‹¤\",\"ì—ì„œ\",\"ìœ¼ë¡œ\",\"ì—ê²Œ\",\"ê¹Œì§€\",\"ë•Œë¬¸\",\"ë•Œë¬¸ì—\",\"ì§€ë§Œ\",\"ê±°ë‚˜\",\n",
        "    \"ë•Œ\",\"ê±°\",\"ìˆ˜\",\"ì •ë§\",\"ë„ˆë¬´\",\"ë§¤ìš°\",\"ì§„ì§œ\",\"ì™„ì „\",\"ê³„ì†\",\"ê·¸ëƒ¥\",\"ì¢€\",\"ì˜\",\n",
        "    \"ë”\",\"ëœ\",\"ë§‰\",\"ë˜\",\"ë“±\",\"ë¼\",\"ë°\",\"ìš”\",\"ë‹ˆë‹¤\",\"ê·¸ë¦°\",\"ì´ìš©\",\"ì‚¬ìš©\",\"ì„œë¹„ìŠ¤\",\n",
        "    \"ì°¨ëŸ‰\",\"ìš´ì „\",\"ì˜¤ëŠ˜\",\"ì–´ì œ\",\"ë‚´ì¼\",\"ì´ë²ˆ\",\"ì €ë²ˆ\",\"ë‹¤ë¥¸\",\"ì—†ê³ \",\"ì—†ë‹¤ê³ \",\n",
        "    \"ì—†ìŒ\",\"ì—†ì´\",\"ì—†ëŠ”\",\"ê·¼ë°\",\"ê·¸ëŸ¼\",\"ì „ì—\",\"ê·¸ë ‡ê²Œ\",\"ì´ê²Œ\",\"ì´ëŸ°\",\"ì´ë ‡ê²Œ\",\n",
        "    \"ì²˜ìŒ\",\"ë°”ë¡œ\",\"ì§€ê¸ˆ\",\"ê²°êµ­\",\"ë¬´ìŠ¨\",\"ì ˆëŒ€\",\"ë§ì´\",\"ì „í˜€\",\"ì•„ë‹ˆ\",\"ì•„ë‹ˆê³ \",\"ì•„ì˜ˆ\",\n",
        "    \"ì•ˆë˜ê³ \",\"ì•ˆë˜ì„œ\",\"ì•ˆëœë‹¤ê³ \",\"ì•ˆë¨\",\"ì•ˆë°›ê³ \",\"ëª»í•˜ê³ \",\"í•´ë„\",\"í–ˆë”ë‹ˆ\",\"ëŒ€í•œ\",\n",
        "    \"ì €ëŠ”\",\"ì œê°€\",\"ë‚´ê°€\",\"íšŒì‚¬\",\"ê³ ê°ì´\",\"í•©ë‹ˆë‹¤\",\"ë¬¸ì´\",\"ì°¨ê°€\",\"ì°¨ë¥¼\",\"ì°¨ëŸ‰ì´\",\n",
        "    \"ì°¨ëŸ‰ì„\",\"ê·¸ë¦°ì¹´\",\"ê·¸ë¦°ì¹´ëŠ”\",\"ì˜ì¹´\",\"ì˜ì¹´ëŠ”\",\"ì˜ì¹´ë¥¼\",\"ì‚¬ìš©í•˜ë‹¤\",\"ê·¸ë ‡ê²Œ\",\n",
        "    \"ê°‘ìê¸°\",\"ìˆì–´ì„œ\",\"ì´ìš©í•˜ë‹¤\",\"ì—†ì–´ì„œ\",\"ê±°ì˜ˆìš”\",\"ê±°ë„¤ìš”\",\"ê±°ê°™ì•„ìš”\",\"ê±°ê°™ìŒ\",\n",
        "    \"ì•„\",\"ì–´\",\"ìŒ\",\"í—\",\"íœ´\",\"ìš°ì™€\",\"ì—íœ´\",\"ì§„ì§œë¡œ\",\"ì™€\",\"í \",\"ì•„ë‹ˆìš”\",\"ë„¤\",\"ì‘\",\n",
        "    \"ê·¸ë˜ì„œ\",\"ê·¸ëŸ¬ë‹ˆê¹Œ\",\"ê·¸ëŸ°ë°\",\"ê·¸ëŸ¼ì—ë„\",\"ê·¸ëŸ¬ë©´\",\"ì•„ë‹ˆë©´\",\"ë•Œë¬¸ì¸ì§€\",\n",
        "    \"ë•Œë¬¸ì¸ì§€ë„\",\"ì™œëƒë©´\",\"ê·¸ë ‡ì§€ë§Œ\",\"ë˜í•œ\",\"ê·¸ë¦¬ê³ ë‚˜ì„œ\",\"ê·¸ë˜ë„\",\"ê·¸ëŸ°ê°€\",\n",
        "    \"ê·¸ë¬ë”ë‹ˆ\",\"ê·¸ë ‡ë‹¤ë³´ë‹ˆ\",\"í•˜ê²Œ\",\"í•˜ë©´ì„œë„\",\"í•˜ë ¤ê³ \",\"í•˜ë ¤ë‹ˆ\",\n",
        "    \"ê¹Œì§€ëŠ”\",\"ì²˜ëŸ¼\",\"ì •ë„\",\"ëŒ€ë¡œ\",\"ë¿\",\"ë”°ë¼\",\"ë§ˆë‹¤\",\"ë¶€í„°\",\"ë§Œí¼\",\"í•˜ë©´ì„œ\",\n",
        "    \"ê·¸ë‚˜ë§ˆ\",\"ì¡°ê¸ˆ\",\"ì‚´ì§\",\"ì•½ê°„\",\"ë˜ê²Œ\",\"ì—„ì²­\",\"ë„ˆë¬´ë‚˜\",\"êµ‰ì¥íˆ\",\"í•­ìƒ\",\n",
        "    \"ê³„ì†í•´ì„œ\",\"ë§¤ë²ˆ\",\"ëŒ€ì²´ë¡œ\",\"ê±°ì˜\",\"ì•„ì£¼\",\"ì™„ì „íˆ\",\"ë„ëŒ€ì²´\",\n",
        "    \"ì¢€ë”\",\"ë¹¨ë¦¬\",\"ëŠ¦ê²Œ\",\"ì²˜ë¦¬\",\"ì œë°œ\",\"ë¬¸ì˜\",\"ìš”ì²­\",\"ë‹µë³€\",\"ë¶ˆí¸\",\n",
        "    \"ë¬¸ì˜í–ˆëŠ”ë°\",\"ë¬¸ì˜ë“œë ¤ìš”\",\"ê°œì„ \",\"í•„ìš”\",\"í•´ê²°\",\"ì¡°ì¹˜\",\"ë¬¸ì œ\",\"ìƒí™©\",\n",
        "    \"ë„ìš”\",\"ìœ¼ë¡œëŠ”\",\"ì—ëŠ”\",\"ì—ê²ŒëŠ”\",\"ë¼ê³ \",\"ì´ë¼ê³ \",\"ë„¤ìš”\",\"ìŠµë‹ˆë‹¤\",\"ëŠ”ë°\",\n",
        "    \"ëŠ”ë°ìš”\",\"í•´ì„œìš”\",\"ê°™ì•„ìš”\",\"í•´ìš”\",\"í•˜ë„¤ìš”\",\"ë„¤ìš”.\",\"ê°™ìŒ\",\"ê±°ì—ìš”\"\n",
        "])\n",
        "\n",
        "\n",
        "# 7. í† í°í™” + ë¶ˆìš©ì–´ ì œê±°\n",
        "def tokenize_and_filter(text: str):\n",
        "    # 1) í† í°í™”\n",
        "    toks = tokenize_ko(text)\n",
        "    # 2) ë¶ˆìš©ì–´ ì œê±° + ê¸¸ì´ í•„í„°\n",
        "    toks = [w for w in toks if (w not in STOPWORDS and len(w) > 1)]\n",
        "    return toks\n",
        "\n",
        "df[\"tokens\"] = df[\"clean_text\"].apply(tokenize_and_filter)\n",
        "\n",
        "# í† í°ì„ ê³µë°± ê¸°ì¤€ ë¬¸ìì—´ë¡œë„ ë§Œë“¤ì–´ë‘ë©´ TF-IDF, ì›Œë“œí´ë¼ìš°ë“œ ë“±ì— í¸í•¨\n",
        "df[\"tokens_str\"] = df[\"tokens\"].apply(lambda xs: \" \".join(xs))\n",
        "\n",
        "print(\"[INFO] ì „ì²˜ë¦¬ ì˜ˆì‹œ 5ê±´:\")\n",
        "print(df[[\"review_text\", \"clean_text\", \"tokens\"]].head().to_string(index=False))\n",
        "\n",
        "\n",
        "# 8. ë‚ ì§œ ì»¬ëŸ¼ í¬ë§· ì •ë¦¬\n",
        "if date_col:\n",
        "    # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ í›„, YYYY-MM-DD í˜•íƒœ ë¬¸ìì—´ë¡œ ì €ì¥\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\").dt.date.astype(str)\n",
        "\n",
        "\n",
        "# 9. ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥\n",
        "df.to_csv(OUTPUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"ì „ì²˜ë¦¬ ì™„ë£Œ! â†’ {OUTPUT_PATH} ë¡œ ì €ì¥ë¨\")"
      ],
      "metadata": {
        "id": "Hej4YjqworWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ê¸/ë¶€ì • ëª¨ë¸ë§\n",
        "- Sentiment_label : \"ê¸ì •', \"ë¶€ì •\"\n",
        "- Sentiment_score: ê·¿ì •í™•ë¥  (0.0 ~ 1.0)"
      ],
      "metadata": {
        "id": "g_8oo6IyFfvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n",
        "\n",
        "# 1. ê¸°ë³¸ ì…‹ì—…\n",
        "import csv\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# ì „ì²˜ë¦¬ ì™„ë£Œëœ ë¦¬ë·° íŒŒì¼\n",
        "INPUT_PATH = \"/content/reviews_preprocessed.csv\"\n",
        "OUTDIR = \"/content/sentiment_out\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "OUTPUT_PATH = os.path.join(OUTDIR, \"reviews_with_sentiment.csv\")\n",
        "\n",
        "\n",
        "# 2. ë°ì´í„° ë¡œë“œ\n",
        "df = pd.read_csv(INPUT_PATH)\n",
        "print(f\"[INFO] ì…ë ¥ ë°ì´í„°: {len(df):,}ê±´\")\n",
        "\n",
        "assert \"review_text\" in df.columns, \"CSVì— 'review_text' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# (ì„ íƒ) ë‚ ì§œ ì»¬ëŸ¼ ìë™ íƒì§€\n",
        "date_col = None\n",
        "for c in df.columns:\n",
        "    if \"date\" in c.lower():\n",
        "        date_col = c\n",
        "        break\n",
        "print(f\"[INFO] ë‚ ì§œ ì»¬ëŸ¼: {date_col}\")\n",
        "\n",
        "\n",
        "# 3. KoELECTRA ê°ì •ë¶„ì„ ëª¨ë¸ ë¡œë“œ\n",
        "MODEL_NAME = \"jaehyeong/koelectra-base-v3-generalized-sentiment-analysis\"\n",
        "\n",
        "print(f\"[INFO] ëª¨ë¸ ë¡œë“œ: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "print(f\"[INFO] device = {device}\")\n",
        "\n",
        "# ë¼ë²¨ ì¸ë±ìŠ¤ í™•ì¸ (ëª¨ë¸ë§ˆë‹¤ 0/1/2ì˜ ì˜ë¯¸ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)\n",
        "id2label = model.config.id2label\n",
        "print(\"[INFO] id2label:\", id2label)\n",
        "\n",
        "# ê¸ì •/ì¤‘ë¦½/ë¶€ì • ì¸ë±ìŠ¤ ì°¾ê¸°\n",
        "pos_idx = neu_idx = neg_idx = None\n",
        "for i, lab in id2label.items():\n",
        "    l = lab.lower()\n",
        "    if \"pos\" in l or \"ê¸ì •\" in l:\n",
        "        pos_idx = i\n",
        "    elif \"neg\" in l or \"ë¶€ì •\" in l:\n",
        "        neg_idx = i\n",
        "\n",
        "# í˜¹ì‹œ ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¸íŒ…\n",
        "if pos_idx is None:\n",
        "    pos_idx = 1\n",
        "if neg_idx is None:\n",
        "    neg_idx = 0\n",
        "\n",
        "print(f\"[INFO] label index - neg:{neg_idx}, neu:{neu_idx}, pos:{pos_idx}\")\n",
        "\n",
        "def map_label(idx: int) -> str:\n",
        "    raw = id2label.get(idx, \"\").lower()\n",
        "    if \"pos\" in raw or \"ê¸ì •\" in raw:\n",
        "        return \"ê¸ì •\"\n",
        "    if \"neg\" in raw or \"ë¶€ì •\" in raw:\n",
        "        return \"ë¶€ì •\"\n",
        "    # í˜¹ì‹œ ì• ë§¤í•˜ë©´ ì¼ë‹¨ ë¶€ì •/ê¸ì •ìœ¼ë¡œ ë‚˜ëˆ”\n",
        "    return \"ê¸ì •\" if idx == pos_idx else \"ë¶€ì •\"\n",
        "\n",
        "\n",
        "# 4. ë°°ì¹˜ ì¶”ë¡  í•¨ìˆ˜ ì •ì˜\n",
        "def predict_batch(texts, batch_size=64, max_len=128):\n",
        "    labels_kr = []\n",
        "    pos_scores = []\n",
        "\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = list(map(str, texts[i:i+batch_size]))\n",
        "\n",
        "        # í† í¬ë‚˜ì´ì¦ˆ\n",
        "        inputs = tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(**inputs).logits\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        # ê° ìƒ˜í”Œë³„ë¡œ í™•ë¥ ê³¼ ë¼ë²¨ ë½‘ê¸°\n",
        "        for p in probs:\n",
        "            # ì› ëª¨ë¸ì˜ argmax ë¼ë²¨\n",
        "            label_idx = int(p.argmax())\n",
        "            labels_kr.append(map_label(label_idx))\n",
        "            # ê¸ì • í™•ë¥  (pos_idx ê¸°ì¤€)\n",
        "            pos_scores.append(float(p[pos_idx]))\n",
        "\n",
        "    return labels_kr, pos_scores\n",
        "\n",
        "\n",
        "# 5. ì‹¤ì œ ê°ì •ë¶„ì„ ìˆ˜í–‰\n",
        "texts = df[\"review_text\"].fillna(\"\").tolist()\n",
        "print(\"[INFO] ê°ì •ë¶„ì„ ì‹œì‘...\")\n",
        "sent_labels, sent_scores = predict_batch(texts, batch_size=96, max_len=128)\n",
        "\n",
        "df[\"Sentiment_label\"] = sent_labels       # \"ê¸ì •/ë¶€ì •\"\n",
        "df[\"Sentiment_score\"] = sent_scores       # ê¸ì • í™•ë¥  (0~1 float)\n",
        "\n",
        "print(\"[INFO] ê°ì •ë¶„ì„ ê²°ê³¼ ì˜ˆì‹œ 5ê±´:\")\n",
        "print(df[[\"review_text\", \"Sentiment_label\", \"Sentiment_score\"]].head().to_string(index=False))\n",
        "\n",
        "\n",
        "# 6. í…ìŠ¤íŠ¸ ì •ë¦¬ (CSVìš© ì•ˆì „ ì²˜ë¦¬)\n",
        "def sanitize_text(s):\n",
        "    s = \"\" if pd.isna(s) else str(s)\n",
        "    return (\n",
        "        s.replace(\"\\r\\n\", \" \")\n",
        "         .replace(\"\\n\", \" \")\n",
        "         .replace(\"\\t\", \" \")\n",
        "         .replace(\"\\x00\", \"\")\n",
        "    )\n",
        "\n",
        "df[\"review_text\"] = df[\"review_text\"].map(sanitize_text)\n",
        "\n",
        "# ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ í¬ë§· í†µì¼\n",
        "if date_col:\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\").dt.date.astype(str)\n",
        "\n",
        "\n",
        "# 7. ì €ì¥í•  ì»¬ëŸ¼ë§Œ ê³¨ë¼ì„œ CSVë¡œ ì €ì¥\n",
        "save_cols = []\n",
        "\n",
        "for col in [\"ID\", \"provider\", \"store\", \"rating\", date_col, \"review_text\",\n",
        "            \"Sentiment_label\", \"Sentiment_score\"]:\n",
        "    if col and col in df.columns:\n",
        "        save_cols.append(col)\n",
        "\n",
        "safe = df[save_cols].copy()\n",
        "\n",
        "safe.to_csv(\n",
        "    OUTPUT_PATH,\n",
        "    index=False,\n",
        "    encoding=\"utf-8-sig\",\n",
        "    quoting=csv.QUOTE_MINIMAL,\n",
        "    lineterminator=\"\\n\"\n",
        ")\n",
        "print(f\"ê°ì •ë¶„ì„ ì™„ë£Œ! â†’ {OUTPUT_PATH} ë¡œ ì €ì¥ë¨\")"
      ],
      "metadata": {
        "id": "BhDpevjZqjCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XAI"
      ],
      "metadata": {
        "id": "b9Sg3Mk0Fxob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. SHAP ì…‹ì—… (masker + í•¨ìˆ˜)"
      ],
      "metadata": {
        "id": "lG1-f0SHsTF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "masker = shap.maskers.Text(tokenizer)\n",
        "\n",
        "def f_shap(texts, max_len=128):\n",
        "    texts = list(map(str, texts))\n",
        "    inputs = tokenizer(\n",
        "        texts,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_len\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs[:, pos_idx]"
      ],
      "metadata": {
        "id": "6UqYQyBksODt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. SHAP Explainer ìƒì„±"
      ],
      "metadata": {
        "id": "eOsm3euosWmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(\n",
        "    f_shap,\n",
        "    masker,\n",
        "    algorithm=\"partition\"\n",
        ")"
      ],
      "metadata": {
        "id": "c9LLsY9psYFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. ë¦¬ë·° ìƒ˜í”Œ ì„ ì • & SHAP ê°’ ê³„ì‚°"
      ],
      "metadata": {
        "id": "1S1CHebrsbr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neg_samples = df[df[\"Sentiment_label\"]==\"ë¶€ì •\"][\"review_text\"].head(2).tolist()\n",
        "pos_samples = df[df[\"Sentiment_label\"]==\"ê¸ì •\"][\"review_text\"].head(2).tolist()\n",
        "\n",
        "sample_texts = neg_samples + pos_samples\n",
        "\n",
        "print(\"ìƒ˜í”Œ ê°œìˆ˜:\", len(sample_texts))\n",
        "\n",
        "shap_values = explainer(\n",
        "    sample_texts,\n",
        "    max_evals=500,\n",
        "    batch_size=8\n",
        ")"
      ],
      "metadata": {
        "id": "ZP_-Ks2XsdIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. ê°œë³„ ë¦¬ë·°ë³„ ë‹¨ì–´ ì¤‘ìš”ë„ ì‹œê°í™”"
      ],
      "metadata": {
        "id": "fuTEpQ_KsfSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, text in enumerate(sample_texts):\n",
        "    score = float(f_shap([text])[0])\n",
        "    print(f\"\\n[ìƒ˜í”Œ {i+1}] ê¸ì •í™•ë¥  = {score:.3f}\")\n",
        "    shap.plots.text(shap_values[i])"
      ],
      "metadata": {
        "id": "6IqrjhJrsgr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ì „ì—­(Global) ë‹¨ì–´ ì¤‘ìš”ë„ ê³„ì‚°\n",
        "- 80ê°œ ë¦¬ë·° ìƒ˜í”Œë§ í›„ SHAP ê³„ì‚°"
      ],
      "metadata": {
        "id": "MBrdDWiKsi1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts_80 = df[\"review_text\"].dropna().sample(80, random_state=42).tolist()\n",
        "shap_values_80 = explainer(texts_80)"
      ],
      "metadata": {
        "id": "-myWQKIWsnWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ê¸ì •/ë¶€ì • í´ë˜ìŠ¤ë³„ SHAP ë¶„í•´"
      ],
      "metadata": {
        "id": "o90YfeL4sm7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sv = shap_values_80[:,:,0]"
      ],
      "metadata": {
        "id": "s7zhCwjTsrM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì „ì²´ í† í°ë³„ í‰ê·  ì˜í–¥ë ¥ ê³„ì‚°"
      ],
      "metadata": {
        "id": "ik21BBLUstYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_importance = sv.abs.mean(0)"
      ],
      "metadata": {
        "id": "N57XTdqRsvcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ìƒìœ„ 20ê°œ ê¸ì •/ë¶€ì • ë‹¨ì–´ ì¶”ì¶œ"
      ],
      "metadata": {
        "id": "5FzNCZFVsyCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sv.data\n",
        "\n",
        "values = global_importance.values\n",
        "\n",
        "# TOP 20 ê¸ì •(ê¸ì • ì˜í–¥ â†‘)\n",
        "idx_top = np.argsort(-values)[:20]\n",
        "top_pos_words = [(tokens[i], float(values[i])) for i in idx_top]\n",
        "\n",
        "# TOP 20 ë¶€ì •(ê¸ì • í™•ë¥ ì„ ê¹ëŠ” ë‹¨ì–´)\n",
        "idx_neg = np.argsort(values)[:20]\n",
        "top_neg_words = [(tokens[i], float(values[i])) for i in idx_neg]\n",
        "\n",
        "print(\"ê¸ì •ì„ ë°€ì–´ì£¼ëŠ” ë‹¨ì–´ TOP 20\")\n",
        "for w, v in top_pos_words:\n",
        "    print(f\"{w:>10s}  +{v:.4f}\")\n",
        "\n",
        "print(\"ë¶€ì •ì„ ë°€ì–´ì£¼ëŠ” ë‹¨ì–´ TOP 20 (ê¸ì •ì„ ê¹ëŠ” ë‹¨ì–´)\")\n",
        "for w, v in top_neg_words:\n",
        "    print(f\"{w:>10s}  -{v:.4f}\")"
      ],
      "metadata": {
        "id": "yXx2sF-as0Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. ì „ì—­ SHAP ë°” ì°¨íŠ¸ ì‹œê°í™”"
      ],
      "metadata": {
        "id": "ZB5trqeSs3mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_top_words(top_items, title):\n",
        "    tokens_ = [t for t, v in top_items]\n",
        "    vals_   = [v for t, v in top_items]\n",
        "\n",
        "    plt.figure(figsize=(7,5))\n",
        "    y = np.arange(len(tokens_))\n",
        "    plt.barh(y, vals_)\n",
        "    plt.yticks(y, tokens_)\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_top_words(top_pos_words, \"ê¸ì • ì˜í–¥ TOP 20 ë‹¨ì–´\")\n",
        "plot_top_words(top_neg_words, \"ë¶€ì • ì˜í–¥ TOP 20 ë‹¨ì–´\")"
      ],
      "metadata": {
        "id": "dxcvrlFLs5dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë¦¬ë·°ë°ì´í„° í‰ì  ë¶„í¬ & ê²€ì¦\n",
        "- í‰ê·  í‰ì  ê³„ì‚°\n",
        "- íŒŒì´ ì°¨íŠ¸ë¡œ ë¹„êµ\n",
        "- ì •ê·œì„± ê²€ì •\n",
        "- ë“±ë¶„ì‚°ì„± ê²€ì •\n",
        "- Mannâ€“Whitney U ê²€ì •"
      ],
      "metadata": {
        "id": "4CdmdBVUHpJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# 1. ë¸Œëœë“œë³„ í‰ê·  í‰ì  ê³„ì‚°\n",
        "mean_scores = df.groupby(\"provider\")[\"rating\"].mean()\n",
        "print(\"ë¸Œëœë“œë³„ í‰ê·  í‰ì :\")\n",
        "print(mean_scores)\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# 2. í‰ì  ë¶„í¬ ì‹œê°í™” (íŒŒì´ ì°¨íŠ¸)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "providers = df[\"provider\"].unique()\n",
        "\n",
        "for i, provider in enumerate(providers):\n",
        "    ratings = df[df[\"provider\"] == provider][\"rating\"]\n",
        "\n",
        "    rating_counts = ratings.value_counts().sort_index()\n",
        "\n",
        "    axes[i].pie(\n",
        "        rating_counts,\n",
        "        labels=rating_counts.index,\n",
        "        autopct=\"%.1f%%\",\n",
        "        startangle=90,\n",
        "        counterclock=False\n",
        "    )\n",
        "    axes[i].set_title(f\"{provider} í‰ì  ë¶„í¬\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# 3. ì •ê·œì„± ê²€ì • (Shapiro-Wilk)\n",
        "gcar_ratings = df[df[\"provider\"] == \"Gcar\"][\"rating\"]\n",
        "socar_ratings = df[df[\"provider\"] == \"Socar\"][\"rating\"]\n",
        "\n",
        "stat_g, p_g = stats.shapiro(gcar_ratings)\n",
        "stat_s, p_s = stats.shapiro(socar_ratings)\n",
        "\n",
        "print(f\"Gcar ì •ê·œì„± p-value : {p_g:.6f}\")\n",
        "print(f\"Socar ì •ê·œì„± p-value : {p_s:.6f}\")\n",
        "\n",
        "if p_g > 0.05 and p_s > 0.05:\n",
        "    print(\"â†’ ë‘ ì§‘ë‹¨ ëª¨ë‘ ì •ê·œì„±ì„ ë§Œì¡±í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ.\")\n",
        "else:\n",
        "    print(\"â†’ ì •ê·œì„± ë¶ˆë§Œì¡± â†’ ë¹„ëª¨ìˆ˜ ê²€ì •(Mann-Whitney) í•„ìš” ê°€ëŠ¥ì„± ë†’ìŒ.\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# 4. ë“±ë¶„ì‚°ì„± ê²€ì • (Levene Test)\n",
        "stat_levene, p_levene = stats.levene(gcar_ratings, socar_ratings)\n",
        "\n",
        "print(f\"ë“±ë¶„ì‚°ì„± p-value : {p_levene:.6f}\")\n",
        "\n",
        "if p_levene > 0.05:\n",
        "    print(\"â†’ ë“±ë¶„ì‚°ì„± ë§Œì¡±.\")\n",
        "else:\n",
        "    print(\"â†’ ë“±ë¶„ì‚°ì„± ë¶ˆë§Œì¡±.\")\n",
        "\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# 5. í‰ê·  ì°¨ì´ ê²€ì •\n",
        "normal = (p_g > 0.05) and (p_s > 0.05)\n",
        "\n",
        "equal_var = (p_levene > 0.05)\n",
        "\n",
        "print(\"=== ìµœì¢… ê²€ì • ì„ íƒ ===\")\n",
        "\n",
        "if normal and equal_var:\n",
        "    print(\"â†’ ë…ë¦½í‘œë³¸ t-test ì‚¬ìš©\")\n",
        "    stat_t, p_t = stats.ttest_ind(\n",
        "        gcar_ratings, socar_ratings,\n",
        "        equal_var=True\n",
        "    )\n",
        "    print(f\"t-test p-value = {p_t:.10f}\")\n",
        "\n",
        "else:\n",
        "    print(\"â†’ Mann-Whitney U ê²€ì • ì‚¬ìš© (ë¹„ëª¨ìˆ˜ ê²€ì •)\")\n",
        "    stat_u, p_u = stats.mannwhitneyu(\n",
        "        gcar_ratings, socar_ratings,\n",
        "        alternative=\"two-sided\"\n",
        "    )\n",
        "    print(f\"Mann-Whitney U p-value = {p_u:.10f}\")"
      ],
      "metadata": {
        "id": "3gNJ6Tn3tXo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gcar & Socarì˜ ê° ì¹´í…Œê³ ë¦¬ì˜ ë¦¬ë·°\n"
      ],
      "metadata": {
        "id": "mS_G2DfeHU4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "df[\"review_text\"] = df[\"review_text\"].astype(str)\n",
        "\n",
        "\n",
        "# 1. ì¹´í…Œê³ ë¦¬ í•„í„° í•¨ìˆ˜ ì •ì˜\n",
        "#    - ì£¼ì–´ì§„ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ” ë¦¬ë·°ë§Œ í•„í„°ë§\n",
        "def filter_reviews_by_keywords(df, text_col, keywords):\n",
        "    pattern = \"|\".join([fr\"{re.escape(kw)}(\\w)?\" for kw in keywords])\n",
        "    mask = df[text_col].str.contains(pattern, flags=re.IGNORECASE, na=False)\n",
        "    return df[mask].copy()\n",
        "\n",
        "\n",
        "# 2. ê³ ê°ì„¼í„° ê´€ë ¨ ë¦¬ë·°ë§Œ ì¶”ì¶œ\n",
        "cs_keywords = [\"ê³ ê°ì„¼í„°\", \"ê³ ê°\", \"ì„¼í„°\", \"ì „í™”\", \"ìƒë‹´\"]\n",
        "\n",
        "df_cs = filter_reviews_by_keywords(df, \"review_text\", cs_keywords)\n",
        "print(f\"[INFO] ê³ ê°ì„¼í„° ê´€ë ¨ ë¦¬ë·° ìˆ˜: {len(df_cs)}\")\n",
        "\n",
        "\n",
        "# 3. ë¸Œëœë“œë³„ í‰ì  ê°œìˆ˜ & ë¹„ìœ¨ ê³„ì‚°\n",
        "rating_counts = (\n",
        "    df_cs\n",
        "    .groupby([\"provider\", \"rating\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "rating_counts[\"ratio\"] = (\n",
        "    rating_counts\n",
        "    .groupby(\"provider\")[\"count\"]\n",
        "    .transform(lambda x: x / x.sum() * 100)\n",
        ")\n",
        "\n",
        "print(rating_counts)\n",
        "\n",
        "\n",
        "# 4. ì‹œê°í™” (ë§‰ëŒ€ê·¸ë˜í”„)\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "ax = sns.barplot(\n",
        "    data=rating_counts,\n",
        "    x=\"rating\",\n",
        "    y=\"ratio\",\n",
        "    hue=\"provider\",\n",
        "    palette=\"Set2\"\n",
        ")\n",
        "\n",
        "plt.title(\"â€˜ê³ ê°ì„¼í„° ê´€ë ¨ í‚¤ì›Œë“œâ€™ ë¦¬ë·°ì˜ í‰ì  ë¹„ìœ¨ (ë¸Œëœë“œë³„)\")\n",
        "plt.xlabel(\"í‰ì \")\n",
        "plt.ylabel(\"ë¹„ìœ¨(%)\")\n",
        "plt.legend(title=\"ë¸Œëœë“œ\")\n",
        "\n",
        "\n",
        "# 5. ë§‰ëŒ€ ìœ„ì— ë¹„ìœ¨ ìˆ«ì í‘œì‹œ\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(\n",
        "        container,\n",
        "        fmt=\"%.1f%%\",\n",
        "        label_type=\"edge\",\n",
        "        fontsize=9,\n",
        "        padding=2\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u_bUaKRKuNLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# G car ì›Œë“œí´ë¼ìš°ë“œ"
      ],
      "metadata": {
        "id": "E9DqkCuXJqT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "from PIL import Image, ImageFilter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "\n",
        "# 1. íŒŒì¼ ê²½ë¡œ & ì˜µì…˜\n",
        "logo_path = \"/content/Miro Image (4).png\"   # Gcar ë¡œê³  ì´ë¯¸ì§€\n",
        "out_png   = \"gcar_wordcloud_dense.png\"\n",
        "\n",
        "# ìµœì¢… ì´ë¯¸ì§€ í•´ìƒë„ (ë†’ì„ìˆ˜ë¡ ì„ ëª… + ê¸€ì ë” ì´˜ì´˜)\n",
        "width, height = 2000, 2000\n",
        "bg_color  = \"white\"\n",
        "\n",
        "\n",
        "# 2. êµµì€ í•œê¸€ í°íŠ¸(Bold) ìë™ íƒìƒ‰\n",
        "CANDIDATE_FONTS = [\n",
        "    \"/System/Library/Fonts/AppleSDGothicNeoB.ttf\",\n",
        "    \"/System/Library/Fonts/AppleSDGothicNeo.ttc\",\n",
        "    \"/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf\",\n",
        "    \"C:/Windows/Fonts/malgunbd.ttf\",\n",
        "    \"C:/Windows/Fonts/malgun.ttf\",\n",
        "]\n",
        "font_path = next((p for p in CANDIDATE_FONTS if os.path.exists(p)), None)\n",
        "if not font_path:\n",
        "    raise FileNotFoundError(\"Bold í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. font_pathë¥¼ ì§ì ‘ ì„¤ì •í•˜ì„¸ìš”.\")\n",
        "\n",
        "\n",
        "# 3. ìƒ‰ìƒ íŒ”ë ˆíŠ¸ (ë„·í”Œë¦­ìŠ¤ ìŠ¤íƒ€ì¼)\n",
        "NETFLIX_PALETTE = [\"#0B0B0B\", \"#B20710\", \"#E50914\", \"#F5F5F1\"]\n",
        "\n",
        "def netflix_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
        "    \"\"\"\n",
        "    ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìƒ‰ìƒ ëœë¤ ì„ íƒ í•¨ìˆ˜\n",
        "    â†’ ë„·í”Œë¦­ìŠ¤ ëŠë‚Œì„ ë‚´ê¸° ìœ„í•œ ì»¬ëŸ¬ ê°€ì¤‘ì¹˜\n",
        "    \"\"\"\n",
        "    return random.choices(\n",
        "        NETFLIX_PALETTE,\n",
        "        weights=[0.38, 0.34, 0.24, 0.04],\n",
        "        k=1\n",
        "    )[0]\n",
        "\n",
        "\n",
        "# 4. ë¡œê³  ë‚´ë¶€ë¥¼ â€˜ê²€ì •=ë‹¨ì–´ ì±„ìš¸ ê³³â€™ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë§ˆìŠ¤í¬ í•¨ìˆ˜\n",
        "def make_mask_inside_black(path, target_size=(2000, 2000),\n",
        "                           threshold=236, pad=30, thicken=3):\n",
        "    \"\"\"\n",
        "    1) RGBA ë¡œê³  â†’ í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ í†µì¼\n",
        "    2) ì •ì‚¬ê°í˜• ìº”ë²„ìŠ¤ì— ì¤‘ì•™ ë°°ì¹˜\n",
        "    3) target_sizeë¡œ ë¦¬ì‚¬ì´ì¦ˆ\n",
        "    4) threshold ê¸°ì¤€ìœ¼ë¡œ ë¡œê³  ë‚´ë¶€ë§Œ ê²€ì •(0), ì™¸ë¶€ëŠ” í°ìƒ‰(255)\n",
        "    5) MinFilterë¡œ ê²€ì • ì˜ì—­ í™•ì¥(ë¡œê³  ë‚´ë¶€ ë©´ì  ë„“í˜€ ì´˜ì´˜í•¨ ê°•í™”)\n",
        "    \"\"\"\n",
        "    img = Image.open(path).convert(\"RGBA\")\n",
        "\n",
        "    # íˆ¬ëª… ë°°ê²½ ì œê±° â†’ í° ë°°ê²½ í•©ì„±\n",
        "    bg = Image.new(\"RGBA\", img.size, (255, 255, 255, 255))\n",
        "    img = Image.alpha_composite(bg, img).convert(\"RGB\")\n",
        "\n",
        "    # ì •ì‚¬ê°í˜• ìº”ë²„ìŠ¤ì— ë¡œê³  ì¤‘ì•™ ë°°ì¹˜\n",
        "    max_side = max(img.size)\n",
        "    canvas = Image.new(\"RGB\", (max_side + pad * 2, max_side + pad * 2), (255, 255, 255))\n",
        "    offset = ((canvas.size[0] - img.size[0]) // 2,\n",
        "              (canvas.size[1] - img.size[1]) // 2)\n",
        "    canvas.paste(img, offset)\n",
        "\n",
        "    # ì›í•˜ëŠ” ì‚¬ì´ì¦ˆë¡œ í™•ëŒ€ (LANCZOS: ê³ í•´ìƒë„ ë¦¬ì‚¬ì´ì¦ˆ)\n",
        "    canvas = canvas.resize(target_size, Image.LANCZOS)\n",
        "\n",
        "    # ê·¸ë ˆì´ ë³€í™˜ â†’ ì–´ë‘ìš´ ì˜ì—­ë§Œ ì¶”ì¶œ\n",
        "    gray = canvas.convert(\"L\").filter(ImageFilter.GaussianBlur(0.8))\n",
        "    arr  = np.array(gray)\n",
        "\n",
        "    # ë¡œê³  ë‚´ë¶€: 0(ê²€ì •), ì™¸ë¶€: 255(í°ìƒ‰)\n",
        "    mask = np.where(arr < threshold, 0, 255).astype(np.uint8)\n",
        "\n",
        "    # ë¡œê³  ë‚´ë¶€ë¥¼ â€˜íŒ½ì°½â€™ì‹œì¼œ ë‹¨ì–´ê°€ ë” ì´˜ì´˜í•˜ê²Œ ë“¤ì–´ê°€ë„ë¡ í•¨\n",
        "    if thicken and thicken >= 3 and thicken % 2 == 1:\n",
        "        pil_mask = Image.fromarray(mask)\n",
        "        for _ in range(2):  # ë„ˆë¬´ ë§ì´ íŒ½ì°½ë˜ë©´ ëª¨ì–‘ì´ ë¬´ë„ˆì§ â†’ ì ë‹¹íˆ 1~2íšŒ\n",
        "            pil_mask = pil_mask.filter(ImageFilter.MinFilter(size=thicken))\n",
        "        mask = np.array(pil_mask)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "# ---- ë§ˆìŠ¤í¬ ìƒì„± ----\n",
        "mask = make_mask_inside_black(\n",
        "    logo_path,\n",
        "    target_size=(width, height),\n",
        "    threshold=238,\n",
        "    pad=40,\n",
        "    thicken=5\n",
        ")\n",
        "\n",
        "\n",
        "# 5. ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
        "freqs = dict([\n",
        "    (\"ì•ˆëœë‹¤\", 2300), (\"ê³ ê°\", 1650), (\"ì„¼í„°\", 1200), (\"ì•„ë‹ˆë‹¤\", 1050), (\"ì•±\", 990),\n",
        "    (\"ì‹œê°„\", 940), (\"ë°˜ë‚©\", 920), (\"ì „í™”\", 880), (\"ì˜ˆì•½\", 860), (\"ì¸ì¦\", 750),\n",
        "    (\"ì“°ë‹¤\", 720), (\"í•´ì£¼ë‹¤\", 690), (\"ì—…ë°ì´íŠ¸\", 660), (\"ë‹¤ì‹œ\", 640), (\"ì˜¤ë¥˜\", 580),\n",
        "    (\"ë“±ë¡\", 610), (\"ë¬¸ì˜\", 600), (\"ì²˜ë¦¬\", 590), (\"ë¡œê·¸ì¸\", 500), (\"ë¬¸ì œ\", 510),\n",
        "    (\"ê°€ë‹¤\", 560), (\"ì—°ë½\", 540), (\"ë¹Œë¦¬ë‹¤\", 520), (\"í™•ì¸\", 480), (\"ì‚¬ê³ \", 470),\n",
        "    (\"ë˜ë‹¤\", 460), (\"ê°€ì…\", 450), (\"ì¿ í°\", 430), (\"ê²°ì œ\", 400), (\"ì·¨ì†Œ\", 390),\n",
        "    (\"ë‚˜ì˜¤ë‹¤\", 380), (\"ì˜¤ë‹¤\", 370), (\"ìƒë‹´\", 350), (\"ëª¨ë¥´ë‹¤\", 340), (\"ì‚¬ëŒ\", 330),\n",
        "    (\"ì–´ë–»ë‹¤\", 320), (\"ì…ë ¥\", 300), (\"ì—°ê²°\", 290), (\"íšŒì›\", 280), (\"í¬ì¸íŠ¸\", 270),\n",
        "    (\"ìƒê°\", 260), (\"í•´ë³´ë‹¤\", 250)\n",
        "])\n",
        "\n",
        "\n",
        "wc = WordCloud(\n",
        "    font_path=font_path,\n",
        "    width=width, height=height,\n",
        "    background_color=bg_color,\n",
        "    mask=mask,\n",
        "    mode=\"RGBA\",\n",
        "\n",
        "    # ì›Œë“œí´ë¼ìš°ë“œ ë°€ë„ ì¡°ì ˆ í•µì‹¬ íŒŒë¼ë¯¸í„°\n",
        "    scale=3,             # í•´ìƒë„ ì¦ê°€ â†’ ê¸€ì ì„ ëª…\n",
        "    max_words=1500,      # ë‹¨ì–´ ìˆ˜ ì¦ê°€ â†’ ë‚´ë¶€ ê³µê°„ ë” ì´˜ì´˜í•˜ê²Œ\n",
        "    min_font_size=10,    # ê³¼ë„í•œ í° ê³µë°± ë°©ì§€\n",
        "    margin=0,            # ê¸€ì ê°„ ì—¬ë°± ìµœì†Œí™”\n",
        "    repeat=True,         # ë¹ˆ ê³µê°„ ë©”ìš°ê¸° ìœ„í•´ ë‹¨ì–´ ë°˜ë³µ í—ˆìš©\n",
        "    relative_scaling=0.35,\n",
        "    prefer_horizontal=0.9,\n",
        "    collocations=False,  # ë‹¨ì–´ìŒ ë¬¶ì„ ë°©ì§€ (ë‹¨ì–´ ë‹¤ì–‘ì„±â†‘)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "wc.generate_from_frequencies(freqs)\n",
        "wc.recolor(color_func=netflix_color_func)\n",
        "\n",
        "\n",
        "# 6. ì‹œê°í™” & ì €ì¥\n",
        "plt.figure(figsize=(9, 9))\n",
        "plt.imshow(wc, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "wc.to_file(out_png)\n",
        "print(\"ì €ì¥ ì™„ë£Œ:\", os.path.abspath(out_png))"
      ],
      "metadata": {
        "id": "5IfuaNWJvGZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ ì‹œê°í™”\n",
        "- ê³ ê°ì„¼í„°Â·ì „í™” ìƒìœ„ í‚¤ì›Œë“œ\n",
        "- ì•±Â·ì˜ˆì•½ ìƒìœ„ í‚¤ì›Œë“œ\n",
        "- ì‹œê°„Â·ë°˜ë‚© ìƒìœ„ í‚¤ì›Œë“œ"
      ],
      "metadata": {
        "id": "r0QmH1N4H8Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# 1. ì¹´í…Œê³ ë¦¬(ê·¸ë£¹) ì •ì˜\n",
        "GROUPS = {\n",
        "    \"ê³ ê°ì„¼í„°Â·ì „í™”\": [\"ê³ ê°ì„¼í„°\", \"ì „í™”\"],\n",
        "    \"ì•±Â·ì˜ˆì•½\":      [\"ì•±\", \"ì–´í”Œ\", \"ì˜ˆì•½\"],\n",
        "    \"ì‹œê°„Â·ë°˜ë‚©\":    [\"ì‹œê°„\", \"ë°˜ë‚©\"],\n",
        "}\n",
        "\n",
        "\n",
        "# 2. ê·¸ë£¹ë³„ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜\n",
        "def extract_top_keywords(df: pd.DataFrame,\n",
        "                         group_name: str,\n",
        "                         seed_keywords: list,\n",
        "                         top_n: int = 15,\n",
        "                         min_len: int = 2,\n",
        "                         min_count: int = 2,\n",
        "                         show_plot: bool = True):\n",
        "    pattern = \"|\".join(map(re.escape, seed_keywords))\n",
        "    sub = df[df[\"clean_text\"].str.contains(pattern, na=False)]\n",
        "    bag = []\n",
        "    seed_set = set(seed_keywords)\n",
        "\n",
        "    for txt in sub[\"clean_text\"]:\n",
        "        for w in tokenize(txt):\n",
        "            if len(w) < min_len:\n",
        "                continue\n",
        "            if w in STOPWORDS:\n",
        "                continue\n",
        "            if w in seed_set:\n",
        "                continue\n",
        "            bag.append(w)\n",
        "\n",
        "    freq = Counter(bag)\n",
        "\n",
        "    if min_count > 1:\n",
        "        freq = Counter({k: v for k, v in freq.items() if v >= min_count})\n",
        "\n",
        "    top = pd.Series(dict(freq.most_common(top_n)), dtype=\"int64\")\n",
        "\n",
        "    if show_plot and not top.empty:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sorted_top = top.sort_values(ascending=True)\n",
        "\n",
        "        ax = sorted_top.plot(kind=\"barh\")\n",
        "        plt.title(f\"[{group_name}] ê´€ë ¨ ë¦¬ë·° ì£¼ìš” í‚¤ì›Œë“œ\", fontsize=12)\n",
        "        plt.xlabel(\"ë¹ˆë„\")\n",
        "        plt.ylabel(\"í‚¤ì›Œë“œ\")\n",
        "\n",
        "        max_val = sorted_top.max()\n",
        "        offset = max_val * 0.01 if max_val > 0 else 0.1\n",
        "        for patch in ax.patches:\n",
        "            width = patch.get_width()\n",
        "            y = patch.get_y() + patch.get_height() / 2\n",
        "            ax.text(\n",
        "                width + offset,\n",
        "                y,\n",
        "                f\"{int(width)}\",\n",
        "                va=\"center\",\n",
        "                ha=\"left\",\n",
        "                fontsize=9\n",
        "            )\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    return top, sub\n",
        "\n",
        "\n",
        "# 3. ê·¸ë£¹ë³„ ì‹¤í–‰ (ê³ ê°ì„¼í„°Â·ì „í™” / ì•±Â·ì˜ˆì•½ / ì‹œê°„Â·ë°˜ë‚©)\n",
        "# ì „ì²˜ë¦¬: ë¶€ì • ë¦¬ë·°ë§Œ ì‚¬ìš© + clean_text ìƒì„±\n",
        "\n",
        "TOP_N     = 15\n",
        "MIN_LEN   = 2\n",
        "MIN_COUNT = 2\n",
        "\n",
        "top_call, sub_call = extract_top_keywords(\n",
        "    neg_df,\n",
        "    group_name=\"ê³ ê°ì„¼í„°Â·ì „í™”\",\n",
        "    seed_keywords=GROUPS[\"ê³ ê°ì„¼í„°Â·ì „í™”\"],\n",
        "    top_n=TOP_N,\n",
        "    min_len=MIN_LEN,\n",
        "    min_count=MIN_COUNT\n",
        ")\n",
        "\n",
        "top_app, sub_app = extract_top_keywords(\n",
        "    neg_df,\n",
        "    group_name=\"ì•±Â·ì˜ˆì•½\",\n",
        "    seed_keywords=GROUPS[\"ì•±Â·ì˜ˆì•½\"],\n",
        "    top_n=TOP_N,\n",
        "    min_len=MIN_LEN,\n",
        "    min_count=MIN_COUNT\n",
        ")\n",
        "\n",
        "top_time, sub_time = extract_top_keywords(\n",
        "    neg_df,\n",
        "    group_name=\"ì‹œê°„Â·ë°˜ë‚©\",\n",
        "    seed_keywords=GROUPS[\"ì‹œê°„Â·ë°˜ë‚©\"],\n",
        "    top_n=TOP_N,\n",
        "    min_len=MIN_LEN,\n",
        "    min_count=MIN_COUNT\n",
        ")\n",
        "\n",
        "\n",
        "# 4. ìƒìœ„ í‚¤ì›Œë“œ í‘œë¡œ í™•ì¸ + ì˜ˆë¬¸ ì¶œë ¥ í•¨ìˆ˜\n",
        "print(\"\\n[ê³ ê°ì„¼í„°Â·ì „í™”] ìƒìœ„ í‚¤ì›Œë“œ\")\n",
        "print(top_call)\n",
        "\n",
        "print(\"\\n[ì•±Â·ì˜ˆì•½] ìƒìœ„ í‚¤ì›Œë“œ\")\n",
        "print(top_app)\n",
        "\n",
        "print(\"\\n[ì‹œê°„Â·ë°˜ë‚©] ìƒìœ„ í‚¤ì›Œë“œ\")\n",
        "print(top_time)\n",
        "\n",
        "\n",
        "def show_examples(sub_df: pd.DataFrame, n: int = 5):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • ê·¸ë£¹(sub_df)ì—ì„œ ì˜ˆì‹œ ë¦¬ë·° nê°œ ì¶œë ¥\n",
        "    \"\"\"\n",
        "    print(sub_df[\"review_text\"].dropna().head(n).to_string(index=False))\n",
        "\n",
        "\n",
        "print(\"\\n[ê³ ê°ì„¼í„°Â·ì „í™”] ì˜ˆë¬¸ ìƒ˜í”Œ:\")\n",
        "show_examples(sub_call, n=5)\n",
        "\n",
        "print(\"\\n[ì•±Â·ì˜ˆì•½] ì˜ˆë¬¸ ìƒ˜í”Œ:\")\n",
        "show_examples(sub_app, n=5)\n",
        "\n",
        "print(\"\\n[ì‹œê°„Â·ë°˜ë‚©] ì˜ˆë¬¸ ìƒ˜í”Œ:\")\n",
        "show_examples(sub_time, n=5)\n"
      ],
      "metadata": {
        "id": "6RyrCyUkDBdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë¶€ì • ë¦¬ë·° í†µê³„\n",
        "\n",
        "- ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ë¹„ìœ¨ì´ 2021â†’2025 ë™ì•ˆ 'ê°ì†Œ ì¶”ì„¸'ì¸ì§€ í†µê³„ì ìœ¼ë¡œ ê²€ì •\n",
        "- ë‘ ê°€ì§€ ë°©ë²• ì‚¬ìš©:\n",
        "    1) ì„ í˜• ì¶”ì„¸ ê²€ì •: OLS + Newey-West(HAC) í‘œì¤€ì˜¤ì°¨\n",
        "       - ê·€ë¬´ê°€ì„¤: ê¸°ìš¸ê¸° Î²1 = 0 (ì¶”ì„¸ ì—†ìŒ)\n",
        "       - ëŒ€ë¦½ê°€ì„¤: ê¸°ìš¸ê¸° Î²1 < 0 (ê°ì†Œ ì¶”ì„¸)\n",
        "    2) Mannâ€“Kendall ë¹„ëª¨ìˆ˜ ê²€ì • (ë‹¨ì¡° ê°ì†Œ ì—¬ë¶€)\n",
        "- ë‹¤ì¤‘ê²€ì •(Benjaminiâ€“Hochberg FDR) ë³´ì • ì ìš©"
      ],
      "metadata": {
        "id": "NuPbC9E2fNrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "from scipy.stats import t as t_dist\n",
        "\n",
        "# 0) ì „ì œ ì¡°ê±´\n",
        "#    â†’ ë©”ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì•„ë˜ ë³€ìˆ˜ê°€ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆì–´ì•¼\n",
        "#       - ratio_A_gcar : ì—°ë„ë³„ ì§€í‘œ A (ì „ì²´ ëŒ€ë¹„ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ 'ë¶€ì •' ë¹„ìœ¨)\n",
        "#       - ratio_B_gcar : ì—°ë„ë³„ ì§€í‘œ B (ë¶€ì • ë¦¬ë·° ë‚´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹„ì¤‘)\n",
        "#       - CAT_PATTERNS : ì¹´í…Œê³ ë¦¬ ì´ë¦„ dict (í‚¤: ì¹´í…Œê³ ë¦¬ëª…)\n",
        "#       - OUT_DIR      : ê²°ê³¼ ì €ì¥ í´ë” ê²½ë¡œ\n",
        "#       - YEAR_MIN_SEL, YEAR_MAX_SEL : ë¶„ì„í•  ì—°ë„ êµ¬ê°„ (ì˜ˆ: 2021~2025)\n",
        "\n",
        "req_vars = [\"ratio_A_gcar\", \"ratio_B_gcar\",\n",
        "            \"CAT_PATTERNS\", \"OUT_DIR\",\n",
        "            \"YEAR_MIN_SEL\", \"YEAR_MAX_SEL\"]\n",
        "\n",
        "missing = [v for v in req_vars if v not in globals()]\n",
        "if missing:\n",
        "    raise RuntimeError(\n",
        "        f\"ë‹¤ìŒ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: {missing}  â†’ ë¨¼ì € ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\"\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# 1) ì„ í˜• ì¶”ì„¸ ê²€ì • (OLS + Newey-West(HAC) í‘œì¤€ì˜¤ì°¨)\n",
        "#    - y: index=ì—°ë„, ê°’=ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨\n",
        "#    - Î²1 < 0 ì¸ì§€ë¥¼ ë³´ëŠ” ë‹¨ì¸¡ ê²€ì • (ê°ì†Œ ì¶”ì„¸?)\n",
        "\n",
        "def ols_trend_test(y: pd.Series) -> dict:\n",
        "    \"\"\"\n",
        "    y : ì—°ë„ë³„ ì§€í‘œ ì‹œê³„ì—´ (index=ì—°ë„, values=ë¹„ìœ¨)\n",
        "    ë°˜í™˜ dict:\n",
        "        - beta1    : ê¸°ìš¸ê¸° ì¶”ì •ì¹˜\n",
        "        - t        : t-í†µê³„ëŸ‰\n",
        "        - p_two    : ì–‘ì¸¡ p-value (Î²1 != 0)\n",
        "        - p_one    : ë‹¨ì¸¡ p-value (Î²1 < 0, ê°ì†Œ ê°€ì„¤)\n",
        "        - n        : ì‚¬ìš©ëœ ê´€ì¸¡ì¹˜ ê°œìˆ˜\n",
        "        - df_resid : ì”ì°¨ ììœ ë„\n",
        "    \"\"\"\n",
        "    y = y.dropna()\n",
        "    if len(y) < 3:\n",
        "        return {\n",
        "            \"beta1\": np.nan, \"t\": np.nan,\n",
        "            \"p_two\": np.nan, \"p_one\": np.nan,\n",
        "            \"n\": len(y), \"df_resid\": np.nan\n",
        "        }\n",
        "\n",
        "    # ì‹œê°„ ì¶•: 0, 1, 2, ... (ì—°ë„ ê°„ê²©ì„ 1ë¡œ ë‘” ë‹¨ìˆœ ì§ì„  íšŒê·€)\n",
        "    t = np.arange(len(y), dtype=float)\n",
        "    X = sm.add_constant(t)  # ìƒìˆ˜í•­ + ê¸°ìš¸ê¸°\n",
        "\n",
        "    model = sm.OLS(y.values, X)\n",
        "\n",
        "    # Newey-West ë˜ê·¸ ìˆ˜: sqrt(T) ì •ë„ë¡œ ë³´ìˆ˜ì ìœ¼ë¡œ ì„¤ì •\n",
        "    T = len(y)\n",
        "    maxlags = max(1, int(np.sqrt(T)))\n",
        "\n",
        "    # HAC(Newey-West) ê³µë¶„ì‚° ì‚¬ìš©\n",
        "    res = model.fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": maxlags})\n",
        "\n",
        "    beta1 = res.params[1]\n",
        "    tval  = res.tvalues[1]\n",
        "    dfree = res.df_resid\n",
        "\n",
        "    # ì–‘ì¸¡ p-value\n",
        "    p_two = res.pvalues[1]\n",
        "    # ë‹¨ì¸¡ p-value (ê°ì†Œ ê°€ì„¤: Î²1 < 0 â†’ ì™¼ìª½ ê¼¬ë¦¬)\n",
        "    p_one = t_dist.cdf(tval, df=dfree)\n",
        "\n",
        "    return {\n",
        "        \"beta1\": float(beta1),\n",
        "        \"t\": float(tval),\n",
        "        \"p_two\": float(p_two),\n",
        "        \"p_one\": float(p_one),\n",
        "        \"n\": int(T),\n",
        "        \"df_resid\": float(dfree),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# 2) Mannâ€“Kendall ë‹¨ì¡° ê°ì†Œ ê²€ì • (ë¹„ëª¨ìˆ˜)\n",
        "#    - ì‹œê³„ì—´ì´ 'ë‹¨ì¡° ê°ì†Œ' ë°©í–¥ì¸ì§€ ë³´ëŠ” ê²€ì •\n",
        "\n",
        "def mann_kendall_decreasing(y: pd.Series) -> dict:\n",
        "    \"\"\"\n",
        "    y : ì—°ë„ë³„ ì§€í‘œ ì‹œê³„ì—´ (Series ë˜ëŠ” array-like)\n",
        "    ë°˜í™˜ dict:\n",
        "        - S      : Mannâ€“Kendall S í†µê³„ëŸ‰\n",
        "        - varS   : Sì˜ ë¶„ì‚°\n",
        "        - z      : í‘œì¤€í™” z í†µê³„ëŸ‰\n",
        "        - p_one  : ë‹¨ì¸¡ p-value (ê°ì†Œ ê°€ì„¤: z < 0 ì¼ìˆ˜ë¡ ìœ ì˜)\n",
        "        - n      : ì‚¬ìš©ëœ ê´€ì¸¡ì¹˜ ê°œìˆ˜\n",
        "    \"\"\"\n",
        "    from scipy.stats import norm\n",
        "\n",
        "    y = y.dropna().values\n",
        "    n = len(y)\n",
        "    if n < 3:\n",
        "        return {\n",
        "            \"S\": np.nan, \"varS\": np.nan,\n",
        "            \"z\": np.nan, \"p_one\": np.nan,\n",
        "            \"n\": n\n",
        "        }\n",
        "\n",
        "    # S í†µê³„ëŸ‰ ê³„ì‚°\n",
        "    S = 0\n",
        "    for i in range(n - 1):\n",
        "        diff = y[i+1:] - y[i]\n",
        "        S += np.sum(np.sign(diff))\n",
        "\n",
        "    # ë¶„ì‚° (ë™ì  ì—†ëŠ” ê¸°ë³¸ ê³µì‹ ì‚¬ìš©)\n",
        "    varS = (n * (n - 1) * (2 * n + 5)) / 18.0\n",
        "\n",
        "    # z ê°’ ê³„ì‚° (S>0, S<0 ì— ë”°ë¼ ë³´ì •)\n",
        "    if S > 0:\n",
        "        z = (S - 1) / np.sqrt(varS)\n",
        "    elif S < 0:\n",
        "        z = (S + 1) / np.sqrt(varS)\n",
        "    else:\n",
        "        z = 0.0\n",
        "\n",
        "    # ë‹¨ì¸¡ p-value (ê°ì†Œ ê°€ì„¤: zê°€ ì‘ì„ìˆ˜ë¡ ìœ ì˜)\n",
        "    p_one = norm.cdf(z)\n",
        "\n",
        "    return {\n",
        "        \"S\": float(S),\n",
        "        \"varS\": float(varS),\n",
        "        \"z\": float(z),\n",
        "        \"p_one\": float(p_one),\n",
        "        \"n\": int(n),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# 3) ì¹´í…Œê³ ë¦¬ë³„ë¡œ A/B ì§€í‘œì— ëŒ€í•´ ì¶”ì„¸ ê²€ì • ì‹¤í–‰\n",
        "#    - A: ì „ì²´ ë¦¬ë·° ì¤‘ 'í•´ë‹¹ ì¹´í…Œê³ ë¦¬ & ë¶€ì •' ë¹„ìœ¨\n",
        "#    - B: ë¶€ì • ë¦¬ë·° ë‚´ë¶€ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹„ì¤‘\n",
        "\n",
        "def run_all_tests(ratio_df: pd.DataFrame, label: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    ratio_df : index=ì—°ë„, columns=ì¹´í…Œê³ ë¦¬ ì´ë¦„ (ë¹„ìœ¨ ê°’)\n",
        "    label    : \"A\" ë˜ëŠ” \"B\" (ì–´ë–¤ ì§€í‘œì¸ì§€ í‘œì‹œìš©)\n",
        "\n",
        "    ë°˜í™˜:\n",
        "        ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ\n",
        "          - ì„ í˜•ì¶”ì„¸(OLS+HAC) ê²°ê³¼\n",
        "          - Mannâ€“Kendall ê²°ê³¼\n",
        "          - FDR ë³´ì • pê°’\n",
        "        ë¥¼ ë‹´ì€ DataFrame\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    # ì •ì˜ëœ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë°˜ë³µ\n",
        "    for cat in CAT_PATTERNS.keys():\n",
        "        if cat not in ratio_df.columns:\n",
        "            continue\n",
        "\n",
        "        # ì„ íƒ ì—°ë„ êµ¬ê°„(YEAR_MIN_SEL~YEAR_MAX_SEL)ë§Œ ì‚¬ìš©\n",
        "        y = ratio_df[cat].copy()\n",
        "        y = y.reindex(range(YEAR_MIN_SEL, YEAR_MAX_SEL + 1))\n",
        "\n",
        "        # 1) ì„ í˜• ì¶”ì„¸ ê²€ì •\n",
        "        lin = ols_trend_test(y)\n",
        "\n",
        "        # 2) Mannâ€“Kendall ë‹¨ì¡° ê°ì†Œ ê²€ì •\n",
        "        mk = mann_kendall_decreasing(y)\n",
        "\n",
        "        rows.append({\n",
        "            \"metric\": label,        # A or B\n",
        "            \"category\": cat,        # ì¹´í…Œê³ ë¦¬ ì´ë¦„\n",
        "            \"n_points\": lin[\"n\"],   # ì‚¬ìš©ëœ ì—°ë„ ê°œìˆ˜\n",
        "            \"beta1_slope\": lin[\"beta1\"],\n",
        "            \"t_stat\": lin[\"t\"],\n",
        "            \"p_one_linear\": lin[\"p_one\"],   # ë‹¨ì¸¡ p (ê°ì†Œ ê°€ì„¤)\n",
        "            \"p_two_linear\": lin[\"p_two\"],   # ì–‘ì¸¡ p\n",
        "            \"mk_S\": mk[\"S\"],\n",
        "            \"mk_z\": mk[\"z\"],\n",
        "            \"p_one_mk\": mk[\"p_one\"],        # ë‹¨ì¸¡ p (ê°ì†Œ ê°€ì„¤)\n",
        "        })\n",
        "\n",
        "    out = pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "    # 3-1) Benjaminiâ€“Hochberg FDR ë³´ì •\n",
        "    #      - ë‹¨ì¸¡ pê°’(p_one_linear, p_one_mk)ì— ëŒ€í•´ q-value ê³„ì‚°\n",
        "\n",
        "    def bh_fdr(pvals: pd.Series) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Benjaminiâ€“Hochberg FDR ë³´ì •\n",
        "        - pvals: ì›ë³¸ p-value Series\n",
        "        - ë°˜í™˜: ê° í•­ëª©ì— ëŒ€í•œ q-value (FDR ë³´ì • í›„ pê°’)\n",
        "        \"\"\"\n",
        "        p = pvals.values.astype(float)\n",
        "        m = np.sum(~np.isnan(p))  # ìœ íš¨ pê°’ ê°œìˆ˜\n",
        "\n",
        "        # NaNì€ ë¬´í•œëŒ€ë¡œ ì·¨ê¸‰í•´ì„œ ë’¤ë¡œ ë°€ê¸°\n",
        "        order = np.argsort(np.where(np.isnan(p), np.inf, p))\n",
        "\n",
        "        ranked = np.empty_like(p, dtype=float)\n",
        "        ranked[:] = np.nan\n",
        "\n",
        "        prev = 0.0\n",
        "        for i, idx in enumerate(order, start=1):\n",
        "            if np.isnan(p[idx]):\n",
        "                continue\n",
        "            # BH: q_i = p_i * (m / rank_i)\n",
        "            val = p[idx] * m / i\n",
        "            prev = max(prev, val)      # monotonic ì¦ê°€ ë³´ì¥\n",
        "            ranked[idx] = min(prev, 1.0)\n",
        "\n",
        "        return pd.Series(ranked, index=pvals.index)\n",
        "\n",
        "    # ì„ í˜• ì¶”ì„¸ / Mannâ€“Kendall ê°ê°ì— ëŒ€í•´ FDR ë³´ì • qê°’ ì¶”ê°€\n",
        "    out[\"q_one_linear\"] = bh_fdr(out[\"p_one_linear\"])\n",
        "    out[\"q_one_mk\"]     = bh_fdr(out[\"p_one_mk\"])\n",
        "\n",
        "    # q<0.05 ì¸ ê²½ìš°ë§Œ \"âœ“\" í‘œì‹œí•´ì„œ í•´ì„í•˜ê¸° ì‰½ê²Œ\n",
        "    out[\"sig_linear(q<0.05)\"] = (out[\"q_one_linear\"] < 0.05).map({True: \"âœ“\", False: \"\"})\n",
        "    out[\"sig_mk(q<0.05)\"]     = (out[\"q_one_mk\"] < 0.05).map({True: \"âœ“\", False: \"\"})\n",
        "\n",
        "    # ë³´ê¸° ì¢‹ê²Œ ì •ë ¬\n",
        "    out = out.sort_values([\"metric\", \"category\"]).reset_index(drop=True)\n",
        "    return out\n",
        "\n",
        "\n",
        "# A/B ì§€í‘œ ê°ê°ì— ëŒ€í•´ ê²€ì • ì‹¤í–‰\n",
        "res_A = run_all_tests(ratio_A_gcar, \"A\")  # ì „ì²´ ëŒ€ë¹„ 'í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¶€ì •' ë¹„ìœ¨\n",
        "res_B = run_all_tests(ratio_B_gcar, \"B\")  # ë¶€ì • ë¦¬ë·° ë‚´ë¶€ ì¹´í…Œê³ ë¦¬ êµ¬ì„±ë¹„\n",
        "res_all = pd.concat([res_A, res_B], axis=0).reset_index(drop=True)\n",
        "\n",
        "# ìš”ì•½ ì¶œë ¥\n",
        "print(\"\\n=== ì¶”ì„¸ ê²€ì • ìš”ì•½ (ë‹¨ì¸¡: 'ê°ì†Œ' ê°€ì„¤ ê¸°ì¤€) ===\")\n",
        "display_cols = [\n",
        "    \"metric\", \"category\", \"n_points\",\n",
        "    \"beta1_slope\", \"t_stat\",\n",
        "    \"p_one_linear\", \"q_one_linear\", \"sig_linear(q<0.05)\",\n",
        "    \"mk_S\", \"mk_z\", \"p_one_mk\", \"q_one_mk\", \"sig_mk(q<0.05)\"\n",
        "]\n",
        "print(res_all[display_cols].round(4))\n",
        "\n",
        "# CSV ì €ì¥\n",
        "csv_path = os.path.join(OUT_DIR, \"Gcar_2021_2025_trend_tests.csv\")\n",
        "res_all.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "print(f\"\\nğŸ“„ ê²€ì • ê²°ê³¼ ì €ì¥: {csv_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 4) ì €ì¥ëœ ê·¸ë˜í”„ PNG íŒŒì¼ ê²€ì¦\n",
        "\n",
        "def verify_saved_images(out_dir: str, required_keywords=None):\n",
        "    \"\"\"\n",
        "    out_dir ë‚´ PNG íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì—… + íŒŒì¼ í¬ê¸° ë¦¬í¬íŠ¸\n",
        "\n",
        "    required_keywords:\n",
        "        - ì˜ˆ: ['ë¶€ì •', 'êµ¬ì„±ë¹„', 'changes_A']\n",
        "        - ì§€ì •í•˜ë©´, íŒŒì¼ëª…ì— ì´ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ëŠ”ì§€ ok ì—¬ë¶€ë¥¼ í•¨ê»˜ í‘œì‹œ\n",
        "    \"\"\"\n",
        "    from glob import glob\n",
        "\n",
        "    pngs = sorted(glob(os.path.join(out_dir, \"*.png\")))\n",
        "    print(f\"\\n[ì´ë¯¸ì§€ ê²€ì¦] í´ë”: {out_dir}\")\n",
        "\n",
        "    if not pngs:\n",
        "        print(\"ì €ì¥ëœ PNGê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return pd.DataFrame(columns=[\"file\", \"size_bytes\", \"ok_keyword\"])\n",
        "\n",
        "    rows = []\n",
        "    for p in pngs:\n",
        "        size = os.path.getsize(p)\n",
        "        ok_kw = True\n",
        "        if required_keywords:\n",
        "            ok_kw = any(k in os.path.basename(p) for k in required_keywords)\n",
        "\n",
        "        rows.append({\n",
        "            \"file\": os.path.basename(p),\n",
        "            \"size_bytes\": size,\n",
        "            \"ok_keyword\": ok_kw,\n",
        "        })\n",
        "\n",
        "    dfv = pd.DataFrame(rows).sort_values(\"file\")\n",
        "    print(dfv.to_string(index=False))\n",
        "    return dfv"
      ],
      "metadata": {
        "id": "_l5yck39Eb8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ë¶€ì • ë¦¬ë·° í†µê³„ ê²€ì • ë° ì‹œê°í™”\n",
        "- ì¹´í…Œê³ ë¦¬\n",
        "  - ê³ ê°ì„¼í„°\n",
        "  - ì•±/ì–´í”Œ\n",
        "  - ë°˜ë‚©/ì˜ˆì•½\n",
        "  - ë²Œê¸ˆ/ê³¼ê¸ˆ/íŒ¨ë„í‹°\n",
        "- ì—°ë„ë³„ ì¶”ì´ ê·¸ë˜í”„\n",
        "- ë¶„ê¸°ë³„ ì¶”ì´ ê·¸ë˜í”„\n",
        "- ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë¶„ê¸°ë³„ ê¼­ì§€ì  ê·¸ë˜í”„\n",
        "- ê°œì„ ìœ¨ ê³„ì‚°"
      ],
      "metadata": {
        "id": "PHNfYXeKjiHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/reviews_sentiment_all.csv\"\n",
        "OUT_DIR = \"/content/outputs_gcar\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "BRAND_TARGET = \"Gcar\"\n",
        "YEAR_MIN_SEL, YEAR_MAX_SEL = 2021, 2025\n",
        "\n",
        "\n",
        "# 1. ë°ì´í„° ë¡œë“œ\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# ê¸°ë³¸ ì»¬ëŸ¼ ì´ë¦„\n",
        "text_col = \"review_text\"\n",
        "date_col = \"review_date\"\n",
        "brand_col = \"provider\"\n",
        "sent_col = \"sentiment_label\"\n",
        "rating_col = \"rating\"\n",
        "\n",
        "# ë‚ ì§œ ì²˜ë¦¬\n",
        "df[\"year\"] = pd.to_datetime(df[date_col], errors=\"coerce\").dt.year\n",
        "df[\"quarter\"] = pd.to_datetime(df[date_col], errors=\"coerce\").dt.quarter\n",
        "\n",
        "# Gcarë§Œ ì‚¬ìš©\n",
        "df = df[df[brand_col].str.contains(\"gcar\", case=False, na=False)].copy()\n",
        "\n",
        "# ì—°ë„ ë²”ìœ„ í•„í„°\n",
        "df = df[df[\"year\"].between(YEAR_MIN_SEL, YEAR_MAX_SEL)]\n",
        "\n",
        "# í…ìŠ¤íŠ¸ ì†Œë¬¸ìí™”\n",
        "df[text_col] = df[text_col].astype(str).str.lower()\n",
        "\n",
        "\n",
        "# 2. ë¶€ì • ë¦¬ë·° í”Œë˜ê·¸\n",
        "NEG = {\"negative\", \"ë¶€ì •\", \"bad\", \"-1\"}\n",
        "\n",
        "def is_negative(row):\n",
        "    if row[sent_col] in NEG:\n",
        "        return True\n",
        "    try:\n",
        "        return float(row[rating_col]) <= 2\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "df[\"is_negative\"] = df.apply(is_negative, axis=1)\n",
        "\n",
        "\n",
        "# 3. ì¹´í…Œê³ ë¦¬ íŒ¨í„´\n",
        "CAT_PATTERNS = {\n",
        "    \"ê³ ê°ì„¼í„°/ì „í™”\": re.compile(r\"(ê³ ê°ì„¼í„°|ìƒë‹´|ì „í™”|ë¬¸ì˜|ì½œì„¼í„°)\"),\n",
        "    \"ì•±/ì–´í”Œ\": re.compile(r\"(ì•±|ì–´í”Œ|ë¡œê·¸ì¸|ì˜¤ë¥˜|ë²„ê·¸|ì—ëŸ¬|ì¸ì¦|ì—…ë°ì´íŠ¸)\"),\n",
        "    \"ë°˜ë‚©/ì˜ˆì•½\": re.compile(r\"(ë°˜ë‚©|ì˜ˆì•½|ì·¨ì†Œ|ì—°ì¥|í™•ì •|ì¢…ë£Œ|ì‹œì‘)\"),\n",
        "    \"ë²Œê¸ˆ/ê³¼ê¸ˆ/íŒ¨ë„í‹°\": re.compile(r\"(ë²Œê¸ˆ|ê³¼ê¸ˆ|íŒ¨ë„í‹°|ìœ„ì•½ê¸ˆ|ì—°ì²´ë£Œ)\")\n",
        "}\n",
        "\n",
        "def match_cat(pattern, text):\n",
        "    return bool(pattern.search(text))\n",
        "\n",
        "# ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ ìƒì„±\n",
        "for cat, pat in CAT_PATTERNS.items():\n",
        "    df[cat] = df[text_col].apply(lambda t: match_cat(pat, t))\n",
        "\n",
        "\n",
        "# 4. ì§€í‘œ A / B ê³„ì‚°\n",
        "#    A: ì „ì²´ ë¦¬ë·° ì¤‘ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¶€ì • ë¹„ìœ¨\n",
        "#    B: ë¶€ì • ë¦¬ë·° ë‚´ë¶€ì—ì„œ ì¹´í…Œê³ ë¦¬ êµ¬ì„±ë¹„\n",
        "\n",
        "# ì¹´í…Œê³ ë¦¬ë³„ ë¶€ì • ì—¬ë¶€\n",
        "neg_cols = []\n",
        "for cat in CAT_PATTERNS.keys():\n",
        "    col = f\"neg__{cat}\"\n",
        "    df[col] = df[\"is_negative\"] & df[cat]\n",
        "    neg_cols.append(col)\n",
        "\n",
        "# ---- ì—°ë„ ê¸°ì¤€ ----\n",
        "grp_all = df.groupby(\"year\")\n",
        "A_num = grp_all[neg_cols].sum()\n",
        "A_num.columns = list(CAT_PATTERNS.keys())\n",
        "\n",
        "A_den = grp_all.size().rename(\"n_all\")\n",
        "ratio_A = (A_num.div(A_den, axis=0)).fillna(0)\n",
        "\n",
        "# ë¶€ì • ë¦¬ë·° ë‚´ë¶€\n",
        "grp_neg = df[df[\"is_negative\"]].groupby(\"year\")\n",
        "B_num = grp_neg[list(CAT_PATTERNS.keys())].sum()\n",
        "B_den = grp_neg.size().rename(\"n_neg\")\n",
        "ratio_B = (B_num.div(B_den, axis=0)).fillna(0)\n",
        "\n",
        "# ---- ë¶„ê¸° ê¸°ì¤€ ----\n",
        "grp_all_q = df.groupby([\"year\", \"quarter\"])\n",
        "A_num_q = grp_all_q[neg_cols].sum()\n",
        "A_num_q.columns = list(CAT_PATTERNS.keys())\n",
        "\n",
        "A_den_q = grp_all_q.size().rename(\"n_all_q\")\n",
        "ratio_A_q = (A_num_q.div(A_den_q, axis=0)).fillna(0)\n",
        "\n",
        "grp_neg_q = df[df[\"is_negative\"]].groupby([\"year\", \"quarter\"])\n",
        "B_num_q = grp_neg_q[list(CAT_PATTERNS.keys())].sum()\n",
        "B_den_q = grp_neg_q.size().rename(\"n_neg_q\")\n",
        "ratio_B_q = (B_num_q.div(B_den_q, axis=0)).fillna(0)\n",
        "\n",
        "\n",
        "# 5. ê·¸ë˜í”„ í•¨ìˆ˜\n",
        "def annotate_points(xs, ys):\n",
        "    ax = plt.gca()\n",
        "    for x, y in zip(xs, ys):\n",
        "        ax.text(x, y, f\"{y*100:.1f}%\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "\n",
        "def plot_year(ratio_sub, title, ylabel):\n",
        "    cats = list(CAT_PATTERNS.keys())\n",
        "    ys = ratio_sub.index.values\n",
        "    for cat in cats:\n",
        "        vals = ratio_sub[cat].values\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.plot(ys, vals, marker=\"o\")\n",
        "        annotate_points(ys, vals)\n",
        "        plt.title(f\"{cat} Â· {title}\")\n",
        "        plt.xlabel(\"ì—°ë„\"); plt.ylabel(ylabel + \" (%)\")\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        fname = f\"{cat}_{title}_year.png\"\n",
        "        plt.savefig(os.path.join(OUT_DIR, fname))\n",
        "        plt.show()\n",
        "\n",
        "def plot_quarter(ratio_sub, title, ylabel):\n",
        "    cats = list(CAT_PATTERNS.keys())\n",
        "    idx = ratio_sub.index\n",
        "    xs = np.arange(len(idx))\n",
        "    labels = [f\"{y}Q{q}\" for (y, q) in idx]\n",
        "\n",
        "    for cat in cats:\n",
        "        vals = ratio_sub[cat].values\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.plot(xs, vals, marker=\"o\")\n",
        "        annotate_points(xs, vals)\n",
        "        plt.xticks(xs, labels, rotation=45)\n",
        "        plt.title(f\"{cat} Â· {title}\")\n",
        "        plt.xlabel(\"ë¶„ê¸°\"); plt.ylabel(ylabel + \" (%)\")\n",
        "        plt.grid(alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        fname = f\"{cat}_{title}_quarter.png\"\n",
        "        plt.savefig(os.path.join(OUT_DIR, fname))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# 6. ê·¸ë˜í”„ ìƒì„±\n",
        "plot_year(ratio_A, \"ì „ì²´ ëŒ€ë¹„ ë¶€ì • ë¹„ìœ¨\", \"ë¹„ìœ¨\")\n",
        "plot_year(ratio_B, \"ë¶€ì • ë‚´ë¶€ êµ¬ì„±ë¹„\", \"êµ¬ì„±ë¹„\")\n",
        "\n",
        "plot_quarter(ratio_A_q, \"ì „ì²´ ëŒ€ë¹„ ë¶€ì • ë¹„ìœ¨ â€” ë¶„ê¸°\", \"ë¹„ìœ¨\")\n",
        "plot_quarter(ratio_B_q, \"ë¶€ì • ë‚´ë¶€ êµ¬ì„±ë¹„ â€” ë¶„ê¸°\", \"êµ¬ì„±ë¹„\")\n",
        "\n",
        "\n",
        "# 7. ê°œì„ ìœ¨ ê³„ì‚° (2021â†’2025)\n",
        "def safe_change(v1, v2):\n",
        "    return ((v2 - v1) / v1.replace(0, np.nan) * 100).replace([np.inf, -np.inf], np.nan).fillna(0).round(1)\n",
        "\n",
        "chg = safe_change(ratio_A.loc[YEAR_MIN_SEL], ratio_A.loc[YEAR_MAX_SEL])\n",
        "chg_df = chg.to_frame(name=\"change_percent(2021â†’2025)\")\n",
        "chg_df.to_csv(os.path.join(OUT_DIR, \"Gcar_changes_A_2021_2025.csv\"), encoding=\"utf-8-sig\")\n",
        "\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.bar(chg.index, chg.values)\n",
        "for i, v in enumerate(chg.values):\n",
        "    plt.text(i, v, f\"{v:+.1f}%\", ha=\"center\", va=\"bottom\")\n",
        "plt.title(\"Gcar ì¹´í…Œê³ ë¦¬ ê°œì„ ìœ¨ (2021â†’2025)\")\n",
        "plt.xticks(rotation=15)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUT_DIR, \"Gcar_changes_A_2021_2025.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(\"ì™„ë£Œ: Gcar ë¶€ì • ë¦¬ë·° í†µê³„ + 4ê°œ ì¹´í…Œê³ ë¦¬ ë¶„ì„ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "ldBo4kUdGEDg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}